<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-module-4-vla-robotics/chapter-3-llm-cognitive-planning" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Chapter 3: LLM-Based Cognitive Planning for ROS 2 | Physical AI &amp; Humanoid Robotics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://muhammad-bilal-hussain.github.io/Book-Physical-AI-Humanoid-Robotics/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://muhammad-bilal-hussain.github.io/Book-Physical-AI-Humanoid-Robotics/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://muhammad-bilal-hussain.github.io/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/chapter-3-llm-cognitive-planning"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Chapter 3: LLM-Based Cognitive Planning for ROS 2 | Physical AI &amp; Humanoid Robotics"><meta data-rh="true" name="description" content="Large Language Models in Robotics Applications"><meta data-rh="true" property="og:description" content="Large Language Models in Robotics Applications"><link data-rh="true" rel="icon" href="/Book-Physical-AI-Humanoid-Robotics/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://muhammad-bilal-hussain.github.io/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/chapter-3-llm-cognitive-planning"><link data-rh="true" rel="alternate" href="https://muhammad-bilal-hussain.github.io/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/chapter-3-llm-cognitive-planning" hreflang="en"><link data-rh="true" rel="alternate" href="https://muhammad-bilal-hussain.github.io/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/chapter-3-llm-cognitive-planning" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Chapter 3: LLM-Based Cognitive Planning for ROS 2","item":"https://muhammad-bilal-hussain.github.io/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/chapter-3-llm-cognitive-planning"}]}</script><link rel="alternate" type="application/rss+xml" href="/Book-Physical-AI-Humanoid-Robotics/blog/rss.xml" title="Physical AI &amp; Humanoid Robotics RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/Book-Physical-AI-Humanoid-Robotics/blog/atom.xml" title="Physical AI &amp; Humanoid Robotics Atom Feed"><link rel="stylesheet" href="/Book-Physical-AI-Humanoid-Robotics/assets/css/styles.9c53a5ce.css">
<script src="/Book-Physical-AI-Humanoid-Robotics/assets/js/runtime~main.697fff44.js" defer="defer"></script>
<script src="/Book-Physical-AI-Humanoid-Robotics/assets/js/main.f7b42da1.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/Book-Physical-AI-Humanoid-Robotics/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/Book-Physical-AI-Humanoid-Robotics/"><div class="navbar__logo"><img src="/Book-Physical-AI-Humanoid-Robotics/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/Book-Physical-AI-Humanoid-Robotics/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI Book</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/Book-Physical-AI-Humanoid-Robotics/docs/docs">Text Book</a><a class="navbar__item navbar__link" href="/Book-Physical-AI-Humanoid-Robotics/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/Muhammad-Bilal-Hussain/Book-Physical-AI-Humanoid-Robotics" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Book-Physical-AI-Humanoid-Robotics/docs/docs"><span title="Physical AI &amp; Humanoid Robotics" class="linkLabel_WmDU">Physical AI &amp; Humanoid Robotics</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Book-Physical-AI-Humanoid-Robotics/docs/module-1-ros2/chapter-1-architecture"><span title="module-1-ros2" class="categoryLinkLabel_W154">module-1-ros2</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/Book-Physical-AI-Humanoid-Robotics/docs/module-2-digital-twin/"><span title="Module 2: The Digital Twin (Gazebo &amp; Unity)" class="categoryLinkLabel_W154">Module 2: The Digital Twin (Gazebo &amp; Unity)</span></a><button aria-label="Expand sidebar category &#x27;Module 2: The Digital Twin (Gazebo &amp; Unity)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Book-Physical-AI-Humanoid-Robotics/docs/module-3-isaac-ai-brain/architecture-diagram"><span title="module-3-isaac-ai-brain" class="categoryLinkLabel_W154">module-3-isaac-ai-brain</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/architecture-diagram"><span title="module-4-vla-robotics" class="categoryLinkLabel_W154">module-4-vla-robotics</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/architecture-diagram"><span title="Architecture Diagram: Vision-Language-Action (VLA) System" class="linkLabel_WmDU">Architecture Diagram: Vision-Language-Action (VLA) System</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/chapter-1-vla-paradigm"><span title="Chapter 1: Vision-Language-Action Paradigm in Robotics" class="linkLabel_WmDU">Chapter 1: Vision-Language-Action Paradigm in Robotics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/chapter-2-voice-to-action"><span title="Chapter 2: Voice-to-Action Pipelines with OpenAI Whisper" class="linkLabel_WmDU">Chapter 2: Voice-to-Action Pipelines with OpenAI Whisper</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/chapter-3-llm-cognitive-planning"><span title="Chapter 3: LLM-Based Cognitive Planning for ROS 2" class="linkLabel_WmDU">Chapter 3: LLM-Based Cognitive Planning for ROS 2</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/chapter-4-autonomous-humanoid"><span title="Chapter 4: Capstone: The Autonomous Humanoid" class="linkLabel_WmDU">Chapter 4: Capstone: The Autonomous Humanoid</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/glossary"><span title="Glossary: Vision-Language-Action (VLA) Robotics" class="linkLabel_WmDU">Glossary: Vision-Language-Action (VLA) Robotics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/references"><span title="References: Vision-Language-Action (VLA) Robotics" class="linkLabel_WmDU">References: Vision-Language-Action (VLA) Robotics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/transition-to-module-4"><span title="Transition to Module 4: Vision-Language-Action (VLA) - The Capstone" class="linkLabel_WmDU">Transition to Module 4: Vision-Language-Action (VLA) - The Capstone</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Book-Physical-AI-Humanoid-Robotics/docs/references/apa-citation-guide"><span title="references" class="categoryLinkLabel_W154">references</span></a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/Book-Physical-AI-Humanoid-Robotics/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">module-4-vla-robotics</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Chapter 3: LLM-Based Cognitive Planning for ROS 2</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Chapter 3: LLM-Based Cognitive Planning for ROS 2</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="large-language-models-in-robotics-applications">Large Language Models in Robotics Applications<a href="#large-language-models-in-robotics-applications" class="hash-link" aria-label="Direct link to Large Language Models in Robotics Applications" title="Direct link to Large Language Models in Robotics Applications" translate="no">​</a></h2>
<p>Large Language Models (LLMs) have revolutionized the field of robotics by providing unprecedented capabilities for natural language understanding and high-level reasoning. These models, trained on vast amounts of text data, can interpret complex human instructions and generate detailed plans for robotic systems.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="capabilities-of-llms-in-robotics">Capabilities of LLMs in Robotics<a href="#capabilities-of-llms-in-robotics" class="hash-link" aria-label="Direct link to Capabilities of LLMs in Robotics" title="Direct link to Capabilities of LLMs in Robotics" translate="no">​</a></h3>
<p>LLMs bring several key capabilities to robotics:</p>
<ol>
<li class=""><strong>Natural Language Understanding</strong>: Ability to comprehend complex, nuanced human instructions</li>
<li class=""><strong>World Knowledge</strong>: Access to vast amounts of general knowledge that can inform robotic decision-making</li>
<li class=""><strong>Reasoning</strong>: Capacity for logical reasoning and problem-solving in novel situations</li>
<li class=""><strong>Generalization</strong>: Ability to apply learned knowledge to new, unseen situations</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="challenges-and-limitations">Challenges and Limitations<a href="#challenges-and-limitations" class="hash-link" aria-label="Direct link to Challenges and Limitations" title="Direct link to Challenges and Limitations" translate="no">​</a></h3>
<p>Despite their impressive capabilities, LLMs face several challenges in robotics applications:</p>
<ul>
<li class=""><strong>Hallucinations</strong>: Tendency to generate plausible-sounding but incorrect information</li>
<li class=""><strong>Lack of Real-time Awareness</strong>: Models trained on static data may not reflect current environmental conditions</li>
<li class=""><strong>Physical Grounding</strong>: Difficulty connecting abstract language concepts to physical reality</li>
<li class=""><strong>Safety and Reliability</strong>: Potential for generating unsafe or infeasible plans</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="mapping-natural-language-to-ros-2-actions">Mapping Natural Language to ROS 2 Actions<a href="#mapping-natural-language-to-ros-2-actions" class="hash-link" aria-label="Direct link to Mapping Natural Language to ROS 2 Actions" title="Direct link to Mapping Natural Language to ROS 2 Actions" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-translation-challenge">The Translation Challenge<a href="#the-translation-challenge" class="hash-link" aria-label="Direct link to The Translation Challenge" title="Direct link to The Translation Challenge" translate="no">​</a></h3>
<p>One of the key challenges in LLM-robotics integration is translating high-level natural language commands into specific ROS 2 actions. This translation process involves several steps:</p>
<ol>
<li class=""><strong>Intent Recognition</strong>: Determining the high-level goal from the natural language command</li>
<li class=""><strong>Entity Extraction</strong>: Identifying relevant objects, locations, and parameters</li>
<li class=""><strong>Action Selection</strong>: Choosing appropriate ROS 2 actions to achieve the goal</li>
<li class=""><strong>Parameter Mapping</strong>: Converting natural language parameters to ROS 2 action parameters</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="example-translation-process">Example Translation Process<a href="#example-translation-process" class="hash-link" aria-label="Direct link to Example Translation Process" title="Direct link to Example Translation Process" translate="no">​</a></h3>
<p>Consider the command &quot;Pick up the red cup from the table and put it in the sink&quot;:</p>
<ol>
<li class=""><strong>Intent Recognition</strong>: Two-part task - pick up object, then place object</li>
<li class=""><strong>Entity Extraction</strong>: Object: &quot;red cup&quot;, Source: &quot;table&quot;, Destination: &quot;sink&quot;</li>
<li class=""><strong>Action Selection</strong>: Use MoveIt! for navigation, Gripper action for pickup, Place action for placement</li>
<li class=""><strong>Parameter Mapping</strong>: Convert &quot;red cup&quot; to object recognition parameters, &quot;table&quot; and &quot;sink&quot; to navigation goals</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="template-based-approaches">Template-Based Approaches<a href="#template-based-approaches" class="hash-link" aria-label="Direct link to Template-Based Approaches" title="Direct link to Template-Based Approaches" translate="no">​</a></h3>
<p>One approach to mapping natural language to ROS 2 actions involves using predefined templates:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Template: &quot;Move [object] from [location1] to [location2]&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Maps to: Navigate to location1 -&gt; Detect object -&gt; Grasp object -&gt; Navigate to location2 -&gt; Release object</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="semantic-parsing">Semantic Parsing<a href="#semantic-parsing" class="hash-link" aria-label="Direct link to Semantic Parsing" title="Direct link to Semantic Parsing" translate="no">​</a></h3>
<p>More sophisticated approaches use semantic parsing to convert natural language into formal representations that can be directly translated to ROS 2 actions. This involves:</p>
<ol>
<li class=""><strong>Syntactic Analysis</strong>: Parsing the sentence structure</li>
<li class=""><strong>Semantic Role Labeling</strong>: Identifying the roles of different entities in the action</li>
<li class=""><strong>Logical Form Generation</strong>: Creating a formal representation of the command</li>
<li class=""><strong>Action Mapping</strong>: Converting the logical form to ROS 2 actions</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="planning-hierarchies-and-task-decomposition">Planning Hierarchies and Task Decomposition<a href="#planning-hierarchies-and-task-decomposition" class="hash-link" aria-label="Direct link to Planning Hierarchies and Task Decomposition" title="Direct link to Planning Hierarchies and Task Decomposition" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="hierarchical-task-networks">Hierarchical Task Networks<a href="#hierarchical-task-networks" class="hash-link" aria-label="Direct link to Hierarchical Task Networks" title="Direct link to Hierarchical Task Networks" translate="no">​</a></h3>
<p>LLMs excel at decomposing complex tasks into hierarchical structures. This decomposition typically follows a top-down approach:</p>
<ol>
<li class=""><strong>High-Level Goals</strong>: Abstract goals specified in natural language</li>
<li class=""><strong>Mid-Level Tasks</strong>: More specific tasks that contribute to achieving the high-level goal</li>
<li class=""><strong>Low-Level Actions</strong>: Specific ROS 2 actions that can be directly executed</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="example-hierarchy">Example Hierarchy<a href="#example-hierarchy" class="hash-link" aria-label="Direct link to Example Hierarchy" title="Direct link to Example Hierarchy" translate="no">​</a></h3>
<p>For the command &quot;Clean the kitchen,&quot; an LLM might generate the following hierarchy:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Level 1: Clean the kitchen</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── Level 2: Clear the counter</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── Level 3: Identify objects on counter</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── Level 3: Pick up dishes</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   └── Level 3: Place dishes in dishwasher</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── Level 2: Wipe the counter</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── Level 3: Navigate to counter</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── Level 3: Grasp cleaning cloth</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   └── Level 3: Execute wiping motions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">└── Level 2: Sweep the floor</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ├── Level 3: Navigate to storage</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ├── Level 3: Grasp broom</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    └── Level 3: Execute sweeping motions</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="dynamic-replanning">Dynamic Replanning<a href="#dynamic-replanning" class="hash-link" aria-label="Direct link to Dynamic Replanning" title="Direct link to Dynamic Replanning" translate="no">​</a></h3>
<p>Hierarchical planning allows for dynamic replanning when unexpected situations arise. If a low-level action fails, the system can attempt alternative approaches at the same level or request higher-level guidance from the LLM.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="safety-and-validation-of-llm-generated-plans">Safety and Validation of LLM-Generated Plans<a href="#safety-and-validation-of-llm-generated-plans" class="hash-link" aria-label="Direct link to Safety and Validation of LLM-Generated Plans" title="Direct link to Safety and Validation of LLM-Generated Plans" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="safety-challenges">Safety Challenges<a href="#safety-challenges" class="hash-link" aria-label="Direct link to Safety Challenges" title="Direct link to Safety Challenges" translate="no">​</a></h3>
<p>LLMs can generate plans that are unsafe or infeasible for robotic systems. Key safety challenges include:</p>
<ul>
<li class=""><strong>Physical Impossibility</strong>: Plans that violate laws of physics or robot kinematics</li>
<li class=""><strong>Environmental Hazards</strong>: Plans that lead the robot into dangerous situations</li>
<li class=""><strong>Social Norms</strong>: Plans that violate social or cultural norms</li>
<li class=""><strong>Resource Constraints</strong>: Plans that exceed robot capabilities</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="validation-approaches">Validation Approaches<a href="#validation-approaches" class="hash-link" aria-label="Direct link to Validation Approaches" title="Direct link to Validation Approaches" translate="no">​</a></h3>
<p>Several approaches can be used to validate LLM-generated plans:</p>
<ol>
<li class=""><strong>Simulation Testing</strong>: Test plans in simulation before execution</li>
<li class=""><strong>Rule-Based Checking</strong>: Apply predefined safety rules to filter plans</li>
<li class=""><strong>Expert Validation</strong>: Use domain experts to review plans</li>
<li class=""><strong>Incremental Execution</strong>: Execute plans in small increments with safety checks</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="safety-first-architecture">Safety-First Architecture<a href="#safety-first-architecture" class="hash-link" aria-label="Direct link to Safety-First Architecture" title="Direct link to Safety-First Architecture" translate="no">​</a></h3>
<p>A safety-first architecture places validation layers between the LLM and the robot:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">[LLM-Generated Plan]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">         ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[Safety Validator]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">         ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[Feasibility Checker]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">         ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[Simulation Validator]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">         ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[Robot Execution]</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="formal-verification">Formal Verification<a href="#formal-verification" class="hash-link" aria-label="Direct link to Formal Verification" title="Direct link to Formal Verification" translate="no">​</a></h3>
<p>For critical applications, formal verification methods can be used to mathematically prove that LLM-generated plans satisfy safety properties. This involves:</p>
<ol>
<li class=""><strong>Formal Specification</strong>: Defining safety properties in formal logic</li>
<li class=""><strong>Plan Translation</strong>: Converting the plan to a formal representation</li>
<li class=""><strong>Verification</strong>: Using automated tools to check safety properties</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="case-studies-of-llm-ros-integration">Case Studies of LLM-ROS Integration<a href="#case-studies-of-llm-ros-integration" class="hash-link" aria-label="Direct link to Case Studies of LLM-ROS Integration" title="Direct link to Case Studies of LLM-ROS Integration" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="rt-2-robotics-transformer-2">RT-2: Robotics Transformer 2<a href="#rt-2-robotics-transformer-2" class="hash-link" aria-label="Direct link to RT-2: Robotics Transformer 2" title="Direct link to RT-2: Robotics Transformer 2" translate="no">​</a></h3>
<p>RT-2 represents a significant advancement in LLM-robotics integration. This system:</p>
<ul>
<li class="">Combines vision-language models with robotic control</li>
<li class="">Learns robotic skills from internet data</li>
<li class="">Can generalize to novel situations and objects</li>
<li class="">Uses transformer architecture for end-to-end learning</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="palm-e">PaLM-E<a href="#palm-e" class="hash-link" aria-label="Direct link to PaLM-E" title="Direct link to PaLM-E" translate="no">​</a></h3>
<p>PaLM-E integrates a large language model with embodied robotic systems:</p>
<ul>
<li class="">Provides contextual understanding for robotic tasks</li>
<li class="">Handles ambiguous or underspecified commands</li>
<li class="">Demonstrates strong performance on complex manipulation tasks</li>
<li class="">Incorporates real-time perception feedback</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="voxposer">VoxPoser<a href="#voxposer" class="hash-link" aria-label="Direct link to VoxPoser" title="Direct link to VoxPoser" translate="no">​</a></h3>
<p>VoxPoser enables robots to manipulate objects based on natural language descriptions:</p>
<ul>
<li class="">Uses 3D spatial reasoning to understand object relationships</li>
<li class="">Generates precise manipulation plans from language descriptions</li>
<li class="">Integrates with ROS 2 for execution</li>
<li class="">Demonstrates strong performance on challenging manipulation tasks</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="cognitive-planning-module-architecture">Cognitive Planning Module Architecture<a href="#cognitive-planning-module-architecture" class="hash-link" aria-label="Direct link to Cognitive Planning Module Architecture" title="Direct link to Cognitive Planning Module Architecture" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="system-components">System Components<a href="#system-components" class="hash-link" aria-label="Direct link to System Components" title="Direct link to System Components" translate="no">​</a></h3>
<p>A typical cognitive planning module for LLM-robotics integration includes:</p>
<ol>
<li class=""><strong>Language Interface</strong>: Handles communication with the LLM</li>
<li class=""><strong>Knowledge Base</strong>: Stores information about the environment and robot capabilities</li>
<li class=""><strong>Planning Engine</strong>: Decomposes high-level goals into executable actions</li>
<li class=""><strong>Validation Layer</strong>: Ensures safety and feasibility of generated plans</li>
<li class=""><strong>Execution Monitor</strong>: Tracks plan execution and handles exceptions</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="architecture-pattern">Architecture Pattern<a href="#architecture-pattern" class="hash-link" aria-label="Direct link to Architecture Pattern" title="Direct link to Architecture Pattern" translate="no">​</a></h3>
<p>The cognitive planning module typically follows this pattern:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">[Language Command]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">         ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[LLM Interpretation]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">         ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[Knowledge Integration]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">         ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[Hierarchical Planning]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">         ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[Safety Validation]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">         ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[ROS 2 Action Generation]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">         ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[Execution Monitoring]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">         ↓</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">[Feedback Integration]</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="knowledge-integration">Knowledge Integration<a href="#knowledge-integration" class="hash-link" aria-label="Direct link to Knowledge Integration" title="Direct link to Knowledge Integration" translate="no">​</a></h3>
<p>The planning module integrates various types of knowledge:</p>
<ul>
<li class=""><strong>World Knowledge</strong>: General knowledge from the LLM</li>
<li class=""><strong>Spatial Knowledge</strong>: Information about object locations and relationships</li>
<li class=""><strong>Robot Knowledge</strong>: Information about robot capabilities and limitations</li>
<li class=""><strong>Task Knowledge</strong>: Information about specific tasks and procedures</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="symbolic-grounding-of-language-in-physical-space">Symbolic Grounding of Language in Physical Space<a href="#symbolic-grounding-of-language-in-physical-space" class="hash-link" aria-label="Direct link to Symbolic Grounding of Language in Physical Space" title="Direct link to Symbolic Grounding of Language in Physical Space" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-grounding-problem">The Grounding Problem<a href="#the-grounding-problem" class="hash-link" aria-label="Direct link to The Grounding Problem" title="Direct link to The Grounding Problem" translate="no">​</a></h3>
<p>Symbolic grounding refers to the challenge of connecting abstract language concepts to physical reality. This is crucial for robotics applications where language commands must be executed in the real world.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="approaches-to-grounding">Approaches to Grounding<a href="#approaches-to-grounding" class="hash-link" aria-label="Direct link to Approaches to Grounding" title="Direct link to Approaches to Grounding" translate="no">​</a></h3>
<p>Several approaches address the grounding problem:</p>
<ol>
<li class=""><strong>Perceptual Grounding</strong>: Connect language to perceptual features (visual, auditory, tactile)</li>
<li class=""><strong>Functional Grounding</strong>: Connect language to functional properties and affordances</li>
<li class=""><strong>Interactive Grounding</strong>: Learn grounding through interaction with the environment</li>
<li class=""><strong>Distributional Grounding</strong>: Use statistical patterns in language and perception</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="vision-language-models">Vision-Language Models<a href="#vision-language-models" class="hash-link" aria-label="Direct link to Vision-Language Models" title="Direct link to Vision-Language Models" translate="no">​</a></h3>
<p>Vision-language models help bridge the gap between language and perception:</p>
<ul>
<li class=""><strong>CLIP</strong>: Connects visual and textual representations</li>
<li class=""><strong>BLIP</strong>: Provides bidirectional vision-language understanding</li>
<li class=""><strong>Florence</strong>: Offers comprehensive vision-language representations</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="spatial-reasoning">Spatial Reasoning<a href="#spatial-reasoning" class="hash-link" aria-label="Direct link to Spatial Reasoning" title="Direct link to Spatial Reasoning" translate="no">​</a></h3>
<p>Spatial reasoning is crucial for grounding language in physical space:</p>
<ul>
<li class=""><strong>Coordinate Systems</strong>: Map language references to spatial coordinates</li>
<li class=""><strong>Topological Relations</strong>: Understand spatial relationships (above, below, next to)</li>
<li class=""><strong>Size and Scale</strong>: Ground language references to physical dimensions</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References" translate="no">​</a></h2>
<ol>
<li class="">OpenAI. (2023). <em>GPT-4 Technical Report</em>. arXiv preprint arXiv:2303.08774.</li>
<li class="">Google Research. (2022). <em>RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</em>. arXiv preprint arXiv:2212.06817.</li>
<li class="">Google Research. (2022). <em>PaLM-E: An Embodied Multimodal Language Model</em>. arXiv preprint arXiv:2203.03560.</li>
<li class="">[To be filled with peer-reviewed papers on LLMs in robotics]</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/Muhammad-Bilal-Hussain/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/chapter-3-llm-cognitive-planning.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/chapter-2-voice-to-action"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Chapter 2: Voice-to-Action Pipelines with OpenAI Whisper</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/chapter-4-autonomous-humanoid"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Chapter 4: Capstone: The Autonomous Humanoid</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#large-language-models-in-robotics-applications" class="table-of-contents__link toc-highlight">Large Language Models in Robotics Applications</a><ul><li><a href="#capabilities-of-llms-in-robotics" class="table-of-contents__link toc-highlight">Capabilities of LLMs in Robotics</a></li><li><a href="#challenges-and-limitations" class="table-of-contents__link toc-highlight">Challenges and Limitations</a></li></ul></li><li><a href="#mapping-natural-language-to-ros-2-actions" class="table-of-contents__link toc-highlight">Mapping Natural Language to ROS 2 Actions</a><ul><li><a href="#the-translation-challenge" class="table-of-contents__link toc-highlight">The Translation Challenge</a></li><li><a href="#example-translation-process" class="table-of-contents__link toc-highlight">Example Translation Process</a></li><li><a href="#template-based-approaches" class="table-of-contents__link toc-highlight">Template-Based Approaches</a></li><li><a href="#semantic-parsing" class="table-of-contents__link toc-highlight">Semantic Parsing</a></li></ul></li><li><a href="#planning-hierarchies-and-task-decomposition" class="table-of-contents__link toc-highlight">Planning Hierarchies and Task Decomposition</a><ul><li><a href="#hierarchical-task-networks" class="table-of-contents__link toc-highlight">Hierarchical Task Networks</a></li><li><a href="#example-hierarchy" class="table-of-contents__link toc-highlight">Example Hierarchy</a></li><li><a href="#dynamic-replanning" class="table-of-contents__link toc-highlight">Dynamic Replanning</a></li></ul></li><li><a href="#safety-and-validation-of-llm-generated-plans" class="table-of-contents__link toc-highlight">Safety and Validation of LLM-Generated Plans</a><ul><li><a href="#safety-challenges" class="table-of-contents__link toc-highlight">Safety Challenges</a></li><li><a href="#validation-approaches" class="table-of-contents__link toc-highlight">Validation Approaches</a></li><li><a href="#safety-first-architecture" class="table-of-contents__link toc-highlight">Safety-First Architecture</a></li><li><a href="#formal-verification" class="table-of-contents__link toc-highlight">Formal Verification</a></li></ul></li><li><a href="#case-studies-of-llm-ros-integration" class="table-of-contents__link toc-highlight">Case Studies of LLM-ROS Integration</a><ul><li><a href="#rt-2-robotics-transformer-2" class="table-of-contents__link toc-highlight">RT-2: Robotics Transformer 2</a></li><li><a href="#palm-e" class="table-of-contents__link toc-highlight">PaLM-E</a></li><li><a href="#voxposer" class="table-of-contents__link toc-highlight">VoxPoser</a></li></ul></li><li><a href="#cognitive-planning-module-architecture" class="table-of-contents__link toc-highlight">Cognitive Planning Module Architecture</a><ul><li><a href="#system-components" class="table-of-contents__link toc-highlight">System Components</a></li><li><a href="#architecture-pattern" class="table-of-contents__link toc-highlight">Architecture Pattern</a></li><li><a href="#knowledge-integration" class="table-of-contents__link toc-highlight">Knowledge Integration</a></li></ul></li><li><a href="#symbolic-grounding-of-language-in-physical-space" class="table-of-contents__link toc-highlight">Symbolic Grounding of Language in Physical Space</a><ul><li><a href="#the-grounding-problem" class="table-of-contents__link toc-highlight">The Grounding Problem</a></li><li><a href="#approaches-to-grounding" class="table-of-contents__link toc-highlight">Approaches to Grounding</a></li><li><a href="#vision-language-models" class="table-of-contents__link toc-highlight">Vision-Language Models</a></li><li><a href="#spatial-reasoning" class="table-of-contents__link toc-highlight">Spatial Reasoning</a></li></ul></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Book-Physical-AI-Humanoid-Robotics/docs/module-1-ros2">Text Book</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Book-Physical-AI-Humanoid-Robotics/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/Muhammad-Bilal-Hussain/Book-Physical-AI-Humanoid-Robotics" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">© 2026 Physical AI Book</div></div></div></footer></div>
</body>
</html>