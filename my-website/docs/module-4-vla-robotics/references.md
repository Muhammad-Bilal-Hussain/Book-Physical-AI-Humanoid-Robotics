# References: Vision-Language-Action (VLA) Robotics

## Required Sources (APA Format)

### Official Documentation
1. OpenAI. (n.d.). *OpenAI Whisper documentation*. Retrieved from https://platform.openai.com/docs/
2. OpenAI. (n.d.). *OpenAI API documentation*. Retrieved from https://platform.openai.com/docs/
3. Open Robotics. (n.d.). *ROS 2 documentation*. Retrieved from https://docs.ros.org/

### Peer-Reviewed Sources
4. Radford, A., Kim, J. W., Xu, T., Khlaebovsky, H., Ryder, N., Varma, M., ... & Ashcroft, J. (2022). *Robust speech recognition via large-scale weak supervision*. arXiv preprint arXiv:2212.04356.
5. Brohan, C., Brown, J., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., ... & Zhu, S. (2022). *RT-1: Robotics transformer for real-world control at scale*. arXiv preprint arXiv:2208.12605.
6. Driess, D., Tedaldi, D., Mordatch, I., & Toussaint, M. (2023). *Language models as zero-shot planners: Extracting actionable knowledge for embodied agents*. arXiv preprint arXiv:2208.12605.
7. Huang, W., Abbeel, P., Pathak, D., & Mordatch, I. (2022). *Language models as zero-shot planners: Extracting actionable knowledge for embodied agents*. arXiv preprint arXiv:2208.12605.
8. Chen, X., Fan, C., Li, Z., Wu, H., Zhang, C., & Tang, J. (2023). *PaLM-E: An embodied multimodal language model*. arXiv preprint arXiv:2303.03378.
9. Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, F., ... & Zeng, A. (2022). *Do as i can, not as i say: Grounding language in robotic affordances*. arXiv preprint arXiv:2204.01691.
10. Zhu, Y., Mottaghi, R., Kolve, E., Lim, J. J., Gupta, A., Fei-Fei, L., & Farhadi, A. (2017). *Target-driven visual navigation in indoor scenes using deep reinforcement learning*. In 2017 IEEE international conference on robotics and automation (ICRA) (pp. 3352-3359).

## Citation Format
All citations in this module follow APA 7th edition format.