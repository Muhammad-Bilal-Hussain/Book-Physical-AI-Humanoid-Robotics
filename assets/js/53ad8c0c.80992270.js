"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[9852],{5908(e,i,n){n.r(i),n.d(i,{assets:()=>t,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-3-isaac-ai-brain/chapter-3-visual-slam","title":"Chapter 3: Visual SLAM for Humanoid Robots","description":"Introduction to Isaac ROS Fundamentals","source":"@site/docs/module-3-isaac-ai-brain/chapter-3-visual-slam.md","sourceDirName":"module-3-isaac-ai-brain","slug":"/module-3-isaac-ai-brain/chapter-3-visual-slam","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-3-isaac-ai-brain/chapter-3-visual-slam","draft":false,"unlisted":false,"editUrl":"https://github.com/Muhammad-Bilal-Hussain/Book-Physical-AI-Humanoid-Robotics/docs/module-3-isaac-ai-brain/chapter-3-visual-slam.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Isaac ROS and Hardware-Accelerated Perception","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-3-isaac-ai-brain/chapter-2-isaac-ros"},"next":{"title":"Chapter 4: Nav2 Path Planning for Bipedal Navigation","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-3-isaac-ai-brain/chapter-4-nav2-path-planning"}}');var o=n(4848),a=n(8453);const r={},l="Chapter 3: Visual SLAM for Humanoid Robots",t={},c=[{value:"Introduction to Isaac ROS Fundamentals",id:"introduction-to-isaac-ros-fundamentals",level:2},{value:"Core Philosophy",id:"core-philosophy",level:3},{value:"Key Components",id:"key-components",level:3},{value:"GPU-Accelerated Computer Vision in Isaac ROS",id:"gpu-accelerated-computer-vision-in-isaac-ros",level:2},{value:"CUDA and TensorRT Integration",id:"cuda-and-tensorrt-integration",level:3},{value:"Performance Improvements",id:"performance-improvements",level:3},{value:"Supported Algorithms",id:"supported-algorithms",level:3},{value:"Perception Pipeline Construction",id:"perception-pipeline-construction",level:2},{value:"Pipeline Architecture",id:"pipeline-architecture",level:3},{value:"Key Design Principles",id:"key-design-principles",level:3},{value:"Implementation Considerations",id:"implementation-considerations",level:3},{value:"Visual SLAM Fundamentals and Visual-Inertial Odometry",id:"visual-slam-fundamentals-and-visual-inertial-odometry",level:2},{value:"Visual SLAM Overview",id:"visual-slam-overview",level:3},{value:"Key Components of Visual SLAM",id:"key-components-of-visual-slam",level:3},{value:"Visual-Inertial Odometry (VIO)",id:"visual-inertial-odometry-vio",level:3},{value:"Advantages of VIO",id:"advantages-of-vio",level:4},{value:"Sensor Fusion",id:"sensor-fusion",level:4},{value:"Loop Closure Detection and Map Optimization",id:"loop-closure-detection-and-map-optimization",level:2},{value:"Loop Closure",id:"loop-closure",level:3},{value:"Detection Methods",id:"detection-methods",level:4},{value:"Challenges",id:"challenges",level:4},{value:"Map Optimization",id:"map-optimization",level:3},{value:"Backend Optimization",id:"backend-optimization",level:4},{value:"SLAM for Humanoid Robots",id:"slam-for-humanoid-robots",level:2},{value:"Challenges Specific to Humanoid Locomotion",id:"challenges-specific-to-humanoid-locomotion",level:3},{value:"Adaptations for Humanoid Platforms",id:"adaptations-for-humanoid-platforms",level:3},{value:"Key Equations and Formulations",id:"key-equations-and-formulations",level:2},{value:"SLAM Problem Formulation",id:"slam-problem-formulation",level:3},{value:"Visual-Inertial Fusion",id:"visual-inertial-fusion",level:3},{value:"References",id:"references",level:2}];function d(e){const i={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(i.header,{children:(0,o.jsx)(i.h1,{id:"chapter-3-visual-slam-for-humanoid-robots",children:"Chapter 3: Visual SLAM for Humanoid Robots"})}),"\n",(0,o.jsx)(i.h2,{id:"introduction-to-isaac-ros-fundamentals",children:"Introduction to Isaac ROS Fundamentals"}),"\n",(0,o.jsx)(i.p,{children:"Isaac ROS is NVIDIA's collection of hardware-accelerated perception and manipulation packages that bridge the gap between NVIDIA's AI and robotics technologies and the Robot Operating System (ROS). It provides optimized implementations of common robotics algorithms that leverage NVIDIA's GPU computing capabilities to achieve real-time performance for demanding applications."}),"\n",(0,o.jsx)(i.h3,{id:"core-philosophy",children:"Core Philosophy"}),"\n",(0,o.jsx)(i.p,{children:"Isaac ROS is designed around the principle of hardware acceleration for robotics perception and manipulation. It takes advantage of NVIDIA's GPU computing platforms to accelerate computationally intensive tasks such as:"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Computer vision algorithms"}),"\n",(0,o.jsx)(i.li,{children:"Deep learning inference"}),"\n",(0,o.jsx)(i.li,{children:"Sensor processing"}),"\n",(0,o.jsx)(i.li,{children:"Graph-based optimization"}),"\n"]}),"\n",(0,o.jsx)(i.h3,{id:"key-components",children:"Key Components"}),"\n",(0,o.jsx)(i.p,{children:"Isaac ROS includes several key components that enable hardware-accelerated robotics:"}),"\n",(0,o.jsxs)(i.ol,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"GEMS (GPU Embedded Multimedia Streaming)"}),": Optimized multimedia streaming capabilities for robotics applications"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Perception Packages"}),": GPU-accelerated implementations of common perception algorithms"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Manipulation Packages"}),": Hardware-accelerated algorithms for robotic manipulation"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Sensor Drivers"}),": Optimized drivers for various sensors that leverage hardware acceleration"]}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"gpu-accelerated-computer-vision-in-isaac-ros",children:"GPU-Accelerated Computer Vision in Isaac ROS"}),"\n",(0,o.jsx)(i.p,{children:"The primary advantage of Isaac ROS lies in its ability to accelerate computer vision algorithms using NVIDIA GPUs. This acceleration is achieved through several mechanisms:"}),"\n",(0,o.jsx)(i.h3,{id:"cuda-and-tensorrt-integration",children:"CUDA and TensorRT Integration"}),"\n",(0,o.jsx)(i.p,{children:"Isaac ROS leverages NVIDIA's CUDA parallel computing platform and TensorRT inference optimizer to accelerate:"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Image processing pipelines"}),"\n",(0,o.jsx)(i.li,{children:"Feature detection and matching"}),"\n",(0,o.jsx)(i.li,{children:"Deep learning inference"}),"\n",(0,o.jsx)(i.li,{children:"3D reconstruction algorithms"}),"\n"]}),"\n",(0,o.jsx)(i.h3,{id:"performance-improvements",children:"Performance Improvements"}),"\n",(0,o.jsx)(i.p,{children:"GPU acceleration in Isaac ROS typically provides:"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"10x-100x speedup"})," for many computer vision algorithms compared to CPU implementations"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Real-time processing"})," for high-resolution imagery and complex algorithms"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Energy efficiency"})," through optimized GPU utilization"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Scalability"})," to handle multiple sensors and algorithms simultaneously"]}),"\n"]}),"\n",(0,o.jsx)(i.h3,{id:"supported-algorithms",children:"Supported Algorithms"}),"\n",(0,o.jsx)(i.p,{children:"Isaac ROS provides GPU-accelerated implementations of common computer vision algorithms:"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Stereo vision and depth estimation"}),"\n",(0,o.jsx)(i.li,{children:"Optical flow computation"}),"\n",(0,o.jsx)(i.li,{children:"Feature detection and description (ORB, SIFT, etc.)"}),"\n",(0,o.jsx)(i.li,{children:"Image rectification and calibration"}),"\n",(0,o.jsx)(i.li,{children:"3D point cloud processing"}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"perception-pipeline-construction",children:"Perception Pipeline Construction"}),"\n",(0,o.jsx)(i.p,{children:"Building an effective perception pipeline in Isaac ROS involves several key steps:"}),"\n",(0,o.jsx)(i.h3,{id:"pipeline-architecture",children:"Pipeline Architecture"}),"\n",(0,o.jsx)(i.p,{children:"A typical Isaac ROS perception pipeline follows this architecture:"}),"\n",(0,o.jsx)(i.pre,{children:(0,o.jsx)(i.code,{children:"[Raw Sensor Data]\n         \u2193\n[Sensor Calibration]\n         \u2193\n[Preprocessing (GPU)]\n         \u2193\n[Feature Extraction (GPU)]\n         \u2193\n[Deep Learning Inference (GPU)]\n         \u2193\n[Post-processing (GPU)]\n         \u2193\n[Fusion and Interpretation]\n         \u2193\n[Semantic Understanding]\n"})}),"\n",(0,o.jsx)(i.h3,{id:"key-design-principles",children:"Key Design Principles"}),"\n",(0,o.jsxs)(i.ol,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"GPU-First Approach"}),": Maximize GPU utilization for computationally intensive tasks"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Low Latency"}),": Minimize processing delays to enable real-time robotics applications"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Robustness"}),": Handle sensor failures and degraded conditions gracefully"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Scalability"}),": Support multiple sensors and processing streams"]}),"\n"]}),"\n",(0,o.jsx)(i.h3,{id:"implementation-considerations",children:"Implementation Considerations"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Memory Management"}),": Efficient GPU memory allocation and reuse"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Pipeline Staging"}),": Proper buffering and synchronization between pipeline stages"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Resource Sharing"}),": Coordination between multiple algorithms using shared resources"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Fallback Mechanisms"}),": CPU-based alternatives when GPU resources are constrained"]}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"visual-slam-fundamentals-and-visual-inertial-odometry",children:"Visual SLAM Fundamentals and Visual-Inertial Odometry"}),"\n",(0,o.jsx)(i.p,{children:"Simultaneous Localization and Mapping (SLAM) is a fundamental capability for autonomous robots, allowing them to navigate unknown environments while building a map of their surroundings. Visual SLAM specifically uses visual sensors (cameras) as the primary input."}),"\n",(0,o.jsx)(i.h3,{id:"visual-slam-overview",children:"Visual SLAM Overview"}),"\n",(0,o.jsx)(i.p,{children:"Visual SLAM algorithms solve the problem of estimating a camera's trajectory while simultaneously reconstructing the 3D structure of the observed environment. The core challenge is that both the camera motion and the scene structure are initially unknown."}),"\n",(0,o.jsx)(i.h3,{id:"key-components-of-visual-slam",children:"Key Components of Visual SLAM"}),"\n",(0,o.jsxs)(i.ol,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Front-End"}),": Responsible for tracking camera motion between frames"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Back-End"}),": Optimizes the estimated trajectory and map over time"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Mapping"}),": Maintains the 3D map of the environment"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Loop Closure"}),": Detects revisits to previously mapped areas"]}),"\n"]}),"\n",(0,o.jsx)(i.h3,{id:"visual-inertial-odometry-vio",children:"Visual-Inertial Odometry (VIO)"}),"\n",(0,o.jsx)(i.p,{children:"Visual-Inertial Odometry combines visual information from cameras with inertial measurements from IMUs to provide more robust and accurate pose estimation:"}),"\n",(0,o.jsx)(i.h4,{id:"advantages-of-vio",children:"Advantages of VIO"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Robustness"}),": Less susceptible to visual aliasing and illumination changes"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Accuracy"}),": Higher precision in pose estimation"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Continuity"}),": IMU provides motion estimates during visual occlusions"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Initialization"}),": Faster and more reliable initialization"]}),"\n"]}),"\n",(0,o.jsx)(i.h4,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,o.jsx)(i.p,{children:"VIO algorithms fuse visual and inertial data through:"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Tightly Coupled Fusion"}),": Direct integration of raw sensor measurements"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Loosely Coupled Fusion"}),": Combination of separately processed visual and inertial estimates"]}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"loop-closure-detection-and-map-optimization",children:"Loop Closure Detection and Map Optimization"}),"\n",(0,o.jsx)(i.h3,{id:"loop-closure",children:"Loop Closure"}),"\n",(0,o.jsx)(i.p,{children:"Loop closure is a critical component of SLAM that addresses the drift accumulation problem:"}),"\n",(0,o.jsx)(i.h4,{id:"detection-methods",children:"Detection Methods"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Appearance-Based"}),": Recognizing places based on visual appearance"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Geometric"}),": Matching 3D structures across visits"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Bag-of-Words"}),": Efficient place recognition using vocabulary trees"]}),"\n"]}),"\n",(0,o.jsx)(i.h4,{id:"challenges",children:"Challenges"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Appearance Change"}),": Lighting, seasonal, and weather variations"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Dynamic Objects"}),": Moving objects that change between visits"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Partial Views"}),": Different viewing angles of the same location"]}),"\n"]}),"\n",(0,o.jsx)(i.h3,{id:"map-optimization",children:"Map Optimization"}),"\n",(0,o.jsx)(i.p,{children:"Map optimization refines the estimated trajectory and map to minimize inconsistencies:"}),"\n",(0,o.jsx)(i.h4,{id:"backend-optimization",children:"Backend Optimization"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Bundle Adjustment"}),": Joint optimization of camera poses and 3D points"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Pose Graph Optimization"}),": Optimization of relative pose constraints"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Sliding Window"}),": Managing computational complexity with limited history"]}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"slam-for-humanoid-robots",children:"SLAM for Humanoid Robots"}),"\n",(0,o.jsx)(i.p,{children:"Humanoid robots present unique challenges for SLAM systems due to their bipedal locomotion and human-like sensing perspective:"}),"\n",(0,o.jsx)(i.h3,{id:"challenges-specific-to-humanoid-locomotion",children:"Challenges Specific to Humanoid Locomotion"}),"\n",(0,o.jsxs)(i.ol,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Dynamic Motion"}),": Constant motion during walking affects sensor readings"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Height Variation"}),": Changing height during gait cycle affects viewpoint"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Body Oscillation"}),": Natural oscillations during walking create motion blur"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Sensor Mounting"}),": Head-mounted sensors move with body dynamics"]}),"\n"]}),"\n",(0,o.jsx)(i.h3,{id:"adaptations-for-humanoid-platforms",children:"Adaptations for Humanoid Platforms"}),"\n",(0,o.jsxs)(i.ol,{children:["\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Motion Compensation"}),": Accounting for body dynamics in SLAM algorithms"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Multi-Sensor Fusion"}),": Combining vision with IMU and other sensors"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Robust Tracking"}),": Handling rapid motion and vibrations"]}),"\n",(0,o.jsxs)(i.li,{children:[(0,o.jsx)(i.strong,{children:"Real-Time Processing"}),": Meeting timing constraints despite computational demands"]}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"key-equations-and-formulations",children:"Key Equations and Formulations"}),"\n",(0,o.jsx)(i.h3,{id:"slam-problem-formulation",children:"SLAM Problem Formulation"}),"\n",(0,o.jsx)(i.p,{children:"The SLAM problem can be formulated as a maximum likelihood estimation:"}),"\n",(0,o.jsx)(i.pre,{children:(0,o.jsx)(i.code,{children:"P(X, M | Z, U) \u221d P(Z | X, M) \xd7 P(U | X) \xd7 P(M) \xd7 P(X)\n"})}),"\n",(0,o.jsx)(i.p,{children:"Where:"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"X: Robot trajectory"}),"\n",(0,o.jsx)(i.li,{children:"M: Map of landmarks"}),"\n",(0,o.jsx)(i.li,{children:"Z: Sensor observations"}),"\n",(0,o.jsx)(i.li,{children:"U: Control inputs"}),"\n"]}),"\n",(0,o.jsx)(i.h3,{id:"visual-inertial-fusion",children:"Visual-Inertial Fusion"}),"\n",(0,o.jsx)(i.p,{children:"The state vector in VIO typically includes:"}),"\n",(0,o.jsx)(i.pre,{children:(0,o.jsx)(i.code,{children:"x = [p, v, q, ba, bg, bg]\n"})}),"\n",(0,o.jsx)(i.p,{children:"Where:"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"p: Position"}),"\n",(0,o.jsx)(i.li,{children:"v: Velocity"}),"\n",(0,o.jsx)(i.li,{children:"q: Orientation (quaternion)"}),"\n",(0,o.jsx)(i.li,{children:"ba: Accelerometer bias"}),"\n",(0,o.jsx)(i.li,{children:"bg: Gyroscope bias"}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"references",children:"References"}),"\n",(0,o.jsxs)(i.ol,{children:["\n",(0,o.jsxs)(i.li,{children:["NVIDIA. (n.d.). ",(0,o.jsx)(i.em,{children:"NVIDIA Isaac ROS documentation"}),". Retrieved from ",(0,o.jsx)(i.a,{href:"https://docs.nvidia.com/isaac/",children:"https://docs.nvidia.com/isaac/"})]}),"\n",(0,o.jsx)(i.li,{children:"Mur-Artal, R., & Tard\xf3s, J. D. (2017). ORB-SLAM2: an open-source SLAM system for monocular, stereo and RGB-D cameras. IEEE Transactions on Robotics."}),"\n",(0,o.jsx)(i.li,{children:"Qin, T., Li, P., & Shen, S. (2018). VINS-mono: A robust and versatile monocular visual-inertial state estimator. IEEE Transactions on Robotics."}),"\n",(0,o.jsxs)(i.li,{children:["Open Robotics. (n.d.). ",(0,o.jsx)(i.em,{children:"ROS 2 documentation"}),". Retrieved from ",(0,o.jsx)(i.a,{href:"https://docs.ros.org/",children:"https://docs.ros.org/"})]}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,a.R)(),...e.components};return i?(0,o.jsx)(i,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,i,n){n.d(i,{R:()=>r,x:()=>l});var s=n(6540);const o={},a=s.createContext(o);function r(e){const i=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),s.createElement(a.Provider,{value:i},e.children)}}}]);