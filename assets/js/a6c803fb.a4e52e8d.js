"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[4860],{8453(n,e,i){i.d(e,{R:()=>r,x:()=>s});var t=i(6540);const o={},a=t.createContext(o);function r(n){const e=t.useContext(a);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:r(n.components),t.createElement(a.Provider,{value:e},n.children)}},9709(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-2-digital-twin/chapter-5-integration/environment-example","title":"Complete Simulation Environment Example: Humanoid Robot in Warehouse","description":"Introduction","source":"@site/docs/module-2-digital-twin/chapter-5-integration/environment-example.md","sourceDirName":"module-2-digital-twin/chapter-5-integration","slug":"/module-2-digital-twin/chapter-5-integration/environment-example","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-2-digital-twin/chapter-5-integration/environment-example","draft":false,"unlisted":false,"editUrl":"https://github.com/Muhammad-Bilal-Hussain/Book-Physical-AI-Humanoid-Robotics/docs/module-2-digital-twin/chapter-5-integration/environment-example.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Data Flow Architecture in Integrated Simulation Environments","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-2-digital-twin/chapter-5-integration/data-flow"},"next":{"title":"Gazebo-Unity Integration Architecture","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-2-digital-twin/chapter-5-integration/gazebo-unity-architecture"}}');var o=i(4848),a=i(8453);const r={},s="Complete Simulation Environment Example: Humanoid Robot in Warehouse",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Scenario Overview",id:"scenario-overview",level:2},{value:"Warehouse Environment",id:"warehouse-environment",level:3},{value:"Robot Platform",id:"robot-platform",level:3},{value:"System Architecture",id:"system-architecture",level:2},{value:"High-Level Architecture",id:"high-level-architecture",level:3},{value:"Implementation Details",id:"implementation-details",level:2},{value:"1. Gazebo Environment Setup",id:"1-gazebo-environment-setup",level:3},{value:"World File (warehouse.world)",id:"world-file-warehouseworld",level:4},{value:"Robot Model (humanoid_robot.urdf)",id:"robot-model-humanoid_roboturdf",level:4},{value:"2. ROS 2 Control System",id:"2-ros-2-control-system",level:3},{value:"Robot State Publisher Configuration",id:"robot-state-publisher-configuration",level:4},{value:"Joint Controller Configuration",id:"joint-controller-configuration",level:4},{value:"Navigation Configuration",id:"navigation-configuration",level:4},{value:"3. Unity Visualization System",id:"3-unity-visualization-system",level:3},{value:"Unity Robot Controller Script",id:"unity-robot-controller-script",level:4},{value:"4. Perception System",id:"4-perception-system",level:3},{value:"Object Detection Node",id:"object-detection-node",level:4},{value:"5. Control System",id:"5-control-system",level:3},{value:"High-Level Task Planner",id:"high-level-task-planner",level:4},{value:"Launch System",id:"launch-system",level:2},{value:"Main Launch File",id:"main-launch-file",level:3},{value:"Validation and Testing",id:"validation-and-testing",level:2},{value:"Simulation Validation",id:"simulation-validation",level:3},{value:"Performance Testing",id:"performance-testing",level:3}];function m(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"complete-simulation-environment-example-humanoid-robot-in-warehouse",children:"Complete Simulation Environment Example: Humanoid Robot in Warehouse"})}),"\n",(0,o.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(e.p,{children:"This chapter presents a comprehensive example of a complete simulation environment for a humanoid robot operating in a warehouse setting. The example integrates all the concepts covered in previous chapters to create a realistic and functional simulation environment that demonstrates the integration of Gazebo physics, Unity visualization, and ROS 2 communication."}),"\n",(0,o.jsx)(e.h2,{id:"scenario-overview",children:"Scenario Overview"}),"\n",(0,o.jsx)(e.h3,{id:"warehouse-environment",children:"Warehouse Environment"}),"\n",(0,o.jsx)(e.p,{children:"Our simulation environment models a warehouse setting where a humanoid robot performs inventory management tasks. The environment includes:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Multiple levels with ramps and stairs"}),"\n",(0,o.jsx)(e.li,{children:"Storage racks with boxes of various sizes"}),"\n",(0,o.jsx)(e.li,{children:"Moving conveyor belts"}),"\n",(0,o.jsx)(e.li,{children:"Human workers performing tasks"}),"\n",(0,o.jsx)(e.li,{children:"Dynamic obstacles that move throughout the environment"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"robot-platform",children:"Robot Platform"}),"\n",(0,o.jsx)(e.p,{children:"The humanoid robot used in this simulation has:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"28 degrees of freedom (DOF)"}),"\n",(0,o.jsx)(e.li,{children:"LiDAR sensors for navigation"}),"\n",(0,o.jsx)(e.li,{children:"RGB-D cameras for object recognition"}),"\n",(0,o.jsx)(e.li,{children:"IMU for balance and orientation"}),"\n",(0,o.jsx)(e.li,{children:"Manipulator arms for picking and placing objects"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,o.jsx)(e.h3,{id:"high-level-architecture",children:"High-Level Architecture"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           Unity                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Visualization  \u2502  \u2502   Interface     \u2502  \u2502   Analytics     \u2502 \u2502\n\u2502  \u2502   (3D Scene)    \u2502  \u2502   (Controls)    \u2502  \u2502   (Metrics)     \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u2502 ROS 2 Communication Layer\n              \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          ROS 2 Core                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   Navigation    \u2502  \u2502   Perception    \u2502  \u2502   Control       \u2502 \u2502\n\u2502  \u2502   (Move Base)   \u2502  \u2502   (Object Det)  \u2502  \u2502   (Controllers) \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u2502 Gazebo Simulation Layer\n              \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          Gazebo                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   Physics       \u2502  \u2502   Sensors       \u2502  \u2502   Environment   \u2502 \u2502\n\u2502  \u2502   (ODE/Bullet)  \u2502  \u2502   (LiDAR/Cam)   \u2502  \u2502   (Models)      \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(e.h2,{id:"implementation-details",children:"Implementation Details"}),"\n",(0,o.jsx)(e.h3,{id:"1-gazebo-environment-setup",children:"1. Gazebo Environment Setup"}),"\n",(0,o.jsx)(e.h4,{id:"world-file-warehouseworld",children:"World File (warehouse.world)"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<sdf version="1.6">\n  <world name="warehouse_world">\n    \x3c!-- Include ground plane --\x3e\n    <include>\n      <uri>model://ground_plane</uri>\n    </include>\n    \n    \x3c!-- Include sky --\x3e\n    <include>\n      <uri>model://sun</uri>\n    </include>\n    \n    \x3c!-- Warehouse structure --\x3e\n    <model name="warehouse_structure">\n      <pose>0 0 0 0 0 0</pose>\n      <static>true</static>\n      \n      \x3c!-- Walls --\x3e\n      <link name="walls">\n        <collision name="walls_collision">\n          <geometry>\n            <box>\n              <size>20 20 6</size>\n            </box>\n          </geometry>\n        </collision>\n        <visual name="walls_visual">\n          <geometry>\n            <box>\n              <size>20 20 6</size>\n            </box>\n          </geometry>\n          <material>\n            <ambient>0.7 0.7 0.7 1</ambient>\n            <diffuse>0.8 0.8 0.8 1</diffuse>\n          </material>\n        </visual>\n      </link>\n    </model>\n    \n    \x3c!-- Storage racks --\x3e\n    <model name="storage_rack_1">\n      <pose>5 0 0 0 0 0</pose>\n      <static>true</static>\n      \x3c!-- Rack definition --\x3e\n      <link name="rack_base">\n        <collision name="rack_collision">\n          <geometry>\n            <box>\n              <size>2 0.5 2</size>\n            </box>\n          </geometry>\n        </collision>\n        <visual name="rack_visual">\n          <geometry>\n            <box>\n              <size>2 0.5 2</size>\n            </box>\n          </geometry>\n        </visual>\n      </link>\n    </model>\n    \n    \x3c!-- Robot spawn location --\x3e\n    <include>\n      <name>humanoid_robot</name>\n      <uri>model://humanoid_robot_model</uri>\n      <pose>0 0 1 0 0 0</pose>\n    </include>\n    \n    \x3c!-- Physics engine --\x3e\n    <physics type="ode">\n      <max_step_size>0.001</max_step_size>\n      <real_time_factor>1.0</real_time_factor>\n      <real_time_update_rate>1000</real_time_update_rate>\n      <ode>\n        <solver>\n          <type>quick</type>\n          <iters>100</iters>\n          <sor>1.3</sor>\n        </solver>\n        <constraints>\n          <cfm>0.0</cfm>\n          <erp>0.2</erp>\n          <contact_max_correcting_vel>100.0</contact_max_correcting_vel>\n          <contact_surface_layer>0.001</contact_surface_layer>\n        </constraints>\n      </ode>\n    </physics>\n    \n    \x3c!-- ROS interface --\x3e\n    <plugin name="gazebo_ros_api_plugin" filename="libgazebo_ros_init.so">\n      <ros>\n        <namespace>/gazebo</namespace>\n      </ros>\n    </plugin>\n  </world>\n</sdf>\n'})}),"\n",(0,o.jsx)(e.h4,{id:"robot-model-humanoid_roboturdf",children:"Robot Model (humanoid_robot.urdf)"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<robot name="humanoid_robot">\n  \x3c!-- Base link --\x3e\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <box size="0.5 0.3 0.8"/>\n      </geometry>\n      <material name="blue">\n        <color rgba="0 0 1 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.5 0.3 0.8"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="50"/>\n      <inertia ixx="1.0" ixy="0.0" ixz="0.0" iyy="1.0" iyz="0.0" izz="1.0"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Torso --\x3e\n  <joint name="torso_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="torso"/>\n    <origin xyz="0 0 0.4"/>\n  </joint>\n\n  <link name="torso">\n    <visual>\n      <geometry>\n        <box size="0.4 0.3 0.6"/>\n      </geometry>\n      <material name="gray">\n        <color rgba="0.5 0.5 0.5 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.4 0.3 0.6"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="20"/>\n      <inertia ixx="0.5" ixy="0.0" ixz="0.0" iyy="0.5" iyz="0.0" izz="0.5"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Head --\x3e\n  <joint name="neck_joint" type="revolute">\n    <parent link="torso"/>\n    <child link="head"/>\n    <origin xyz="0 0 0.3"/>\n    <axis xyz="0 0 1"/>\n    <limit lower="-1.57" upper="1.57" effort="100" velocity="1"/>\n  </joint>\n\n  <link name="head">\n    <visual>\n      <geometry>\n        <sphere radius="0.15"/>\n      </geometry>\n      <material name="white">\n        <color rgba="1 1 1 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <sphere radius="0.15"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="2"/>\n      <inertia ixx="0.01" ixy="0.0" ixz="0.0" iyy="0.01" iyz="0.0" izz="0.01"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Left Arm --\x3e\n  <joint name="left_shoulder_joint" type="revolute">\n    <parent link="torso"/>\n    <child link="left_upper_arm"/>\n    <origin xyz="0.25 0 0.1" rpy="0 0 0"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="-1.57" upper="1.57" effort="50" velocity="1"/>\n  </joint>\n\n  <link name="left_upper_arm">\n    <visual>\n      <geometry>\n        <cylinder length="0.4" radius="0.05"/>\n      </geometry>\n      <material name="red">\n        <color rgba="1 0 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.4" radius="0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="3"/>\n      <inertia ixx="0.02" ixy="0.0" ixz="0.0" iyy="0.02" iyz="0.0" izz="0.001"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Right Arm --\x3e\n  <joint name="right_shoulder_joint" type="revolute">\n    <parent link="torso"/>\n    <child link="right_upper_arm"/>\n    <origin xyz="-0.25 0 0.1" rpy="0 0 0"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="-1.57" upper="1.57" effort="50" velocity="1"/>\n  </joint>\n\n  <link name="right_upper_arm">\n    <visual>\n      <geometry>\n        <cylinder length="0.4" radius="0.05"/>\n      </geometry>\n      <material name="green">\n        <color rgba="0 1 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.4" radius="0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="3"/>\n      <inertia ixx="0.02" ixy="0.0" ixz="0.0" iyy="0.02" iyz="0.0" izz="0.001"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Sensors --\x3e\n  \x3c!-- LiDAR --\x3e\n  <joint name="lidar_mount_joint" type="fixed">\n    <parent link="head"/>\n    <child link="lidar_link"/>\n    <origin xyz="0 0 0.1"/>\n  </joint>\n\n  <link name="lidar_link">\n    <inertial>\n      <mass value="0.5"/>\n      <inertia ixx="0.001" ixy="0.0" ixz="0.0" iyy="0.001" iyz="0.0" izz="0.001"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Camera --\x3e\n  <joint name="camera_mount_joint" type="fixed">\n    <parent link="head"/>\n    <child link="camera_link"/>\n    <origin xyz="0.1 0 0"/>\n  </joint>\n\n  <link name="camera_link">\n    <inertial>\n      <mass value="0.1"/>\n      <inertia ixx="0.0001" ixy="0.0" ixz="0.0" iyy="0.0001" iyz="0.0" izz="0.0001"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Gazebo plugins --\x3e\n  <gazebo reference="lidar_link">\n    <sensor name="lidar_sensor" type="ray">\n      <pose>0 0 0 0 0 0</pose>\n      <ray>\n        <scan>\n          <horizontal>\n            <samples>360</samples>\n            <resolution>1</resolution>\n            <min_angle>-3.14159</min_angle>\n            <max_angle>3.14159</max_angle>\n          </horizontal>\n        </scan>\n        <range>\n          <min>0.1</min>\n          <max>10.0</max>\n          <resolution>0.01</resolution>\n        </range>\n      </ray>\n      <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\n        <ros>\n          <namespace>/humanoid_robot</namespace>\n          <remapping>~/out:=scan</remapping>\n        </ros>\n        <output_type>sensor_msgs/LaserScan</output_type>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  <gazebo reference="camera_link">\n    <sensor name="camera_sensor" type="camera">\n      <camera>\n        <horizontal_fov>1.047</horizontal_fov>\n        <image>\n          <width>640</width>\n          <height>480</height>\n          <format>R8G8B8</format>\n        </image>\n        <clip>\n          <near>0.1</near>\n          <far>10</far>\n        </clip>\n      </camera>\n      <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n        <ros>\n          <namespace>/humanoid_robot</namespace>\n          <remapping>image_raw:=camera/image_raw</remapping>\n          <remapping>camera_info:=camera/camera_info</remapping>\n        </ros>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  <gazebo reference="base_link">\n    <sensor name="imu_sensor" type="imu">\n      <always_on>true</always_on>\n      <update_rate>100</update_rate>\n      <plugin name="imu_plugin" filename="libgazebo_ros_imu.so">\n        <ros>\n          <namespace>/humanoid_robot</namespace>\n          <remapping>~/out:=imu/data</remapping>\n        </ros>\n        <initial_orientation_as_reference>false</initial_orientation_as_reference>\n      </plugin>\n    </sensor>\n  </gazebo>\n</robot>\n'})}),"\n",(0,o.jsx)(e.h3,{id:"2-ros-2-control-system",children:"2. ROS 2 Control System"}),"\n",(0,o.jsx)(e.h4,{id:"robot-state-publisher-configuration",children:"Robot State Publisher Configuration"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-yaml",children:"# config/robot_state_publisher.yaml\n/**:\n  ros__parameters:\n    use_sim_time: true\n    publish_frequency: 50.0\n"})}),"\n",(0,o.jsx)(e.h4,{id:"joint-controller-configuration",children:"Joint Controller Configuration"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-yaml",children:"# config/joint_controllers.yaml\ncontroller_manager:\n  ros__parameters:\n    update_rate: 100  # Hz\n\n    joint_trajectory_controller:\n      type: joint_trajectory_controller/JointTrajectoryController\n\njoint_trajectory_controller:\n  ros__parameters:\n    joints:\n      - neck_joint\n      - left_shoulder_joint\n      - right_shoulder_joint\n    interface_name: position\n"})}),"\n",(0,o.jsx)(e.h4,{id:"navigation-configuration",children:"Navigation Configuration"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-yaml",children:'# config/navigation.yaml\nbt_navigator:\n  ros__parameters:\n    use_sim_time: true\n    global_frame: map\n    robot_base_frame: base_link\n    odom_topic: /odom\n    bt_xml_filename: "navigate_w_replanning_and_recovery.xml"\n    default_server_timeout: 20\n    enable_groot_monitoring: True\n    groot_zmq_publisher_port: 1666\n    groot_zmq_server_port: 1667\n    goal_check_tolerance: 0.25\n\ncontroller_server:\n  ros__parameters:\n    use_sim_time: true\n    controller_frequency: 20.0\n    min_x_velocity_threshold: 0.001\n    min_y_velocity_threshold: 0.5\n    min_theta_velocity_threshold: 0.001\n    progress_checker_plugin: "progress_checker"\n    goal_checker_plugin: "goal_checker"\n    controller_plugins: ["FollowPath"]\n\n    # DWB parameters\n    FollowPath:\n      plugin: "dwb_core::DWBLocalPlanner"\n      debug_trajectory_details: True\n      min_vel_x: 0.0\n      min_vel_y: 0.0\n      max_vel_x: 0.5\n      max_vel_y: 0.0\n      max_vel_theta: 1.0\n      min_speed_xy: 0.0\n      max_speed_xy: 0.5\n      min_speed_theta: 0.0\n      acc_lim_x: 2.5\n      acc_lim_y: 0.0\n      acc_lim_theta: 3.2\n      decel_lim_x: -2.5\n      decel_lim_y: 0.0\n      decel_lim_theta: -3.2\n      vx_samples: 20\n      vy_samples: 0\n      vtheta_samples: 40\n      sim_time: 1.7\n      linear_granularity: 0.05\n      angular_granularity: 0.1\n      transform_tolerance: 0.2\n      xy_goal_tolerance: 0.25\n      trans_stopped_velocity: 0.25\n      short_circuit_trajectory_evaluation: True\n      stateful: True\n      critics: ["RotateToGoal", "Oscillation", "BaseObstacle", "GoalAlign", "PathAlign", "PathDist", "GoalDist"]\n      BaseObstacle.scale: 0.02\n      PathAlign.scale: 32.0\n      PathAlign.forward_point_distance: 0.1\n      GoalAlign.scale: 24.0\n      GoalAlign.forward_point_distance: 0.1\n      PathDist.scale: 32.0\n      GoalDist.scale: 24.0\n      RotateToGoal.scale: 32.0\n      RotateToGoal.slowing_factor: 5.0\n      RotateToGoal.lookahead_time: -1.0\n'})}),"\n",(0,o.jsx)(e.h3,{id:"3-unity-visualization-system",children:"3. Unity Visualization System"}),"\n",(0,o.jsx)(e.h4,{id:"unity-robot-controller-script",children:"Unity Robot Controller Script"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Sensor_msgs;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Geometry_msgs;\nusing System.Collections.Generic;\n\npublic class HumanoidRobotController : MonoBehaviour\n{\n    [Header("ROS Connection")]\n    public string rosIpAddress = "127.0.0.1";\n    public int rosPort = 10000;\n    \n    [Header("Robot Configuration")]\n    public Transform neckJoint;\n    public Transform leftShoulderJoint;\n    public Transform rightShoulderJoint;\n    \n    [Header("Sensors")]\n    public GameObject lidarPointcloud;\n    public Camera robotCamera;\n    \n    private ROSConnection ros;\n    private Dictionary<string, Transform> jointMap;\n    \n    void Start()\n    {\n        // Initialize ROS connection\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.Initialize(rosIpAddress, rosPort);\n        \n        // Create joint mapping\n        jointMap = new Dictionary<string, Transform>\n        {\n            {"neck_joint", neckJoint},\n            {"left_shoulder_joint", leftShoulderJoint},\n            {"right_shoulder_joint", rightShoulderJoint}\n        };\n        \n        // Subscribe to joint states\n        ros.Subscribe<JointStateMsg>("humanoid_robot/joint_states", OnJointStateReceived);\n        \n        // Subscribe to sensor data\n        ros.Subscribe<LaserScanMsg>("humanoid_robot/scan", OnLidarDataReceived);\n        ros.Subscribe<ImageMsg>("humanoid_robot/camera/image_raw", OnCameraDataReceived);\n    }\n    \n    void OnJointStateReceived(JointStateMsg jointState)\n    {\n        // Update robot joints based on received state\n        for (int i = 0; i < jointState.name.Length; i++)\n        {\n            string jointName = jointState.name[i];\n            double position = jointState.position[i];\n            \n            if (jointMap.ContainsKey(jointName))\n            {\n                Transform joint = jointMap[jointName];\n                \n                // Apply position based on joint type\n                if (jointName.Contains("neck"))\n                {\n                    joint.localRotation = Quaternion.Euler(0, (float)position * Mathf.Rad2Deg, 0);\n                }\n                else if (jointName.Contains("shoulder"))\n                {\n                    joint.localRotation = Quaternion.Euler(0, 0, (float)position * Mathf.Rad2Deg);\n                }\n            }\n        }\n    }\n    \n    void OnLidarDataReceived(LaserScanMsg scan)\n    {\n        // Update LiDAR visualization\n        UpdateLidarVisualization(scan);\n    }\n    \n    void OnCameraDataReceived(ImageMsg image)\n    {\n        // Update camera feed\n        UpdateCameraFeed(image);\n    }\n    \n    void UpdateLidarVisualization(LaserScanMsg scan)\n    {\n        // Create or update point cloud visualization\n        if (lidarPointcloud != null)\n        {\n            // Convert scan data to points and visualize\n            // This is a simplified example\n            for (int i = 0; i < Mathf.Min(scan.ranges.Length, 100); i++) // Limit for performance\n            {\n                float angle = scan.angle_min + i * scan.angle_increment;\n                float distance = (float)scan.ranges[i];\n                \n                if (distance < scan.range_max && distance > scan.range_min)\n                {\n                    Vector3 pointPos = new Vector3(\n                        distance * Mathf.Cos(angle),\n                        0,\n                        distance * Mathf.Sin(angle)\n                    );\n                    \n                    // Create a small sphere to represent the point\n                    GameObject point = GameObject.CreatePrimitive(PrimitiveType.Sphere);\n                    point.transform.SetParent(lidarPointcloud.transform);\n                    point.transform.localPosition = pointPos;\n                    point.transform.localScale = Vector3.one * 0.05f;\n                    point.GetComponent<Renderer>().material.color = Color.red;\n                    \n                    // Destroy after a few seconds to prevent accumulation\n                    Destroy(point, 2.0f);\n                }\n            }\n        }\n    }\n    \n    void UpdateCameraFeed(ImageMsg image)\n    {\n        // Update the robot camera feed\n        // This would typically involve converting the image message to a texture\n        // and applying it to a material or UI element\n    }\n    \n    void OnDestroy()\n    {\n        if (ros != null)\n            ros.Disconnect();\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h3,{id:"4-perception-system",children:"4. Perception System"}),"\n",(0,o.jsx)(e.h4,{id:"object-detection-node",children:"Object Detection Node"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan\nfrom visualization_msgs.msg import MarkerArray, Marker\nfrom geometry_msgs.msg import Point\nimport numpy as np\nimport cv2\nfrom cv_bridge import CvBridge\n\nclass ObjectDetectionNode(Node):\n    def __init__(self):\n        super().__init__('object_detection_node')\n        \n        # Initialize CV bridge\n        self.bridge = CvBridge()\n        \n        # Create subscribers\n        self.image_sub = self.create_subscription(\n            Image, \n            'humanoid_robot/camera/image_raw', \n            self.image_callback, \n            10\n        )\n        \n        self.scan_sub = self.create_subscription(\n            LaserScan,\n            'humanoid_robot/scan',\n            self.scan_callback,\n            10\n        )\n        \n        # Create publishers\n        self.marker_pub = self.create_publisher(\n            MarkerArray,\n            'detected_objects',\n            10\n        )\n        \n        # Object detection parameters\n        self.min_distance = 0.5  # meters\n        self.max_distance = 5.0  # meters\n        \n    def image_callback(self, msg):\n        try:\n            # Convert ROS image to OpenCV image\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n            \n            # Perform object detection (simplified example)\n            # In a real system, this would use a trained model\n            detected_objects = self.detect_objects(cv_image)\n            \n            # Publish detected objects as markers\n            self.publish_markers(detected_objects)\n            \n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n    \n    def scan_callback(self, msg):\n        # Process LiDAR data for object detection\n        # This is a simplified example\n        ranges = np.array(msg.ranges)\n        angles = np.arange(msg.angle_min, msg.angle_max, msg.angle_increment)\n        \n        # Filter out invalid ranges\n        valid_indices = (ranges > msg.range_min) & (ranges < msg.range_max)\n        valid_ranges = ranges[valid_indices]\n        valid_angles = angles[valid_indices]\n        \n        # Convert to Cartesian coordinates\n        x_points = valid_ranges * np.cos(valid_angles)\n        y_points = valid_ranges * np.sin(valid_angles)\n        \n        # Simple clustering to detect objects\n        objects = self.cluster_points(x_points, y_points)\n        \n        # Publish LiDAR-based object detections\n        self.publish_lidar_markers(objects)\n    \n    def detect_objects(self, image):\n        # Simplified object detection using color thresholding\n        # In a real system, this would use a trained model\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        \n        # Define color ranges for different objects\n        lower_red = np.array([0, 50, 50])\n        upper_red = np.array([10, 255, 255])\n        \n        lower_blue = np.array([100, 50, 50])\n        upper_blue = np.array([130, 255, 255])\n        \n        # Create masks\n        mask_red = cv2.inRange(hsv, lower_red, upper_red)\n        mask_blue = cv2.inRange(hsv, lower_blue, upper_blue)\n        \n        # Find contours\n        contours_red, _ = cv2.findContours(mask_red, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        contours_blue, _ = cv2.findContours(mask_blue, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        \n        objects = []\n        \n        # Process red objects\n        for contour in contours_red:\n            if cv2.contourArea(contour) > 100:  # Minimum area threshold\n                M = cv2.moments(contour)\n                if M[\"m00\"] != 0:\n                    cx = int(M[\"m10\"] / M[\"m00\"])\n                    cy = int(M[\"m01\"] / M[\"m00\"])\n                    objects.append({'type': 'red_object', 'x': cx, 'y': cy})\n        \n        # Process blue objects\n        for contour in contours_blue:\n            if cv2.contourArea(contour) > 100:  # Minimum area threshold\n                M = cv2.moments(contour)\n                if M[\"m00\"] != 0:\n                    cx = int(M[\"m10\"] / M[\"m00\"])\n                    cy = int(M[\"m01\"] / M[\"m00\"])\n                    objects.append({'type': 'blue_object', 'x': cx, 'y': cy})\n        \n        return objects\n    \n    def cluster_points(self, x_points, y_points):\n        # Simple clustering algorithm to group LiDAR points into objects\n        # This is a simplified implementation\n        objects = []\n        \n        # For simplicity, we'll just group nearby points\n        # A real implementation would use a more sophisticated clustering algorithm\n        for i in range(len(x_points)):\n            # Check if this point is close to any existing object\n            assigned = False\n            for obj in objects:\n                dist = np.sqrt((x_points[i] - obj['x'])**2 + (y_points[i] - obj['y'])**2)\n                if dist < 0.3:  # 30cm threshold\n                    # Update object center\n                    obj['x'] = (obj['x'] + x_points[i]) / 2\n                    obj['y'] = (obj['y'] + y_points[i]) / 2\n                    obj['count'] += 1\n                    assigned = True\n                    break\n            \n            if not assigned:\n                objects.append({\n                    'x': x_points[i],\n                    'y': y_points[i],\n                    'count': 1\n                })\n        \n        return objects\n    \n    def publish_markers(self, objects):\n        marker_array = MarkerArray()\n        \n        for i, obj in enumerate(objects):\n            marker = Marker()\n            marker.header.frame_id = \"camera_link\"\n            marker.header.stamp = self.get_clock().now().to_msg()\n            marker.ns = \"objects\"\n            marker.id = i\n            marker.type = Marker.CUBE\n            marker.action = Marker.ADD\n            \n            # Set position (simplified - would need depth information)\n            marker.pose.position.x = 1.0  # Placeholder depth\n            marker.pose.position.y = (obj['x'] - 320) * 0.001  # Convert pixel to meters\n            marker.pose.position.z = (obj['y'] - 240) * 0.001  # Convert pixel to meters\n            \n            marker.pose.orientation.x = 0.0\n            marker.pose.orientation.y = 0.0\n            marker.pose.orientation.z = 0.0\n            marker.pose.orientation.w = 1.0\n            \n            # Set size\n            marker.scale.x = 0.2\n            marker.scale.y = 0.2\n            marker.scale.z = 0.2\n            \n            # Set color based on object type\n            if obj['type'] == 'red_object':\n                marker.color.r = 1.0\n                marker.color.g = 0.0\n                marker.color.b = 0.0\n            else:  # blue_object\n                marker.color.r = 0.0\n                marker.color.g = 0.0\n                marker.color.b = 1.0\n            \n            marker.color.a = 1.0\n            \n            marker_array.markers.append(marker)\n        \n        self.marker_pub.publish(marker_array)\n    \n    def publish_lidar_markers(self, objects):\n        # Publish LiDAR-based object markers\n        marker_array = MarkerArray()\n        \n        for i, obj in enumerate(objects):\n            marker = Marker()\n            marker.header.frame_id = \"base_link\"\n            marker.header.stamp = self.get_clock().now().to_msg()\n            marker.ns = \"lidar_objects\"\n            marker.id = i + 1000  # Different ID range\n            marker.type = Marker.CYLINDER\n            marker.action = Marker.ADD\n            \n            # Set position\n            marker.pose.position.x = obj['x']\n            marker.pose.position.y = obj['y']\n            marker.pose.position.z = 0.5  # Half the height of a typical object\n            \n            marker.pose.orientation.x = 0.0\n            marker.pose.orientation.y = 0.0\n            marker.pose.orientation.z = 0.0\n            marker.pose.orientation.w = 1.0\n            \n            # Set size\n            marker.scale.x = 0.3  # Diameter\n            marker.scale.y = 0.3  # Diameter\n            marker.scale.z = 1.0  # Height\n            \n            # Set color\n            marker.color.r = 0.0\n            marker.color.g = 1.0\n            marker.color.b = 0.0\n            marker.color.a = 0.8\n            \n            marker_array.markers.append(marker)\n        \n        self.marker_pub.publish(marker_array)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ObjectDetectionNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(e.h3,{id:"5-control-system",children:"5. Control System"}),"\n",(0,o.jsx)(e.h4,{id:"high-level-task-planner",children:"High-Level Task Planner"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\nfrom action_msgs.msg import GoalStatus\nfrom nav2_msgs.action import NavigateToPose\nfrom rclpy.action import ActionClient\nimport time\nimport random\n\nclass WarehouseTaskPlanner(Node):\n    def __init__(self):\n        super().__init__('warehouse_task_planner')\n        \n        # Create action client for navigation\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n        \n        # Define warehouse locations\n        self.locations = {\n            'entrance': {'x': 0.0, 'y': 0.0, 'theta': 0.0},\n            'aisle_1': {'x': 5.0, 'y': 2.0, 'theta': 0.0},\n            'aisle_2': {'x': 5.0, 'y': -2.0, 'theta': 0.0},\n            'packing_station': {'x': -3.0, 'y': 0.0, 'theta': 1.57},\n            'shipping_dock': {'x': -8.0, 'y': 0.0, 'theta': 3.14}\n        }\n        \n        # Schedule periodic task assignment\n        self.timer = self.create_timer(10.0, self.assign_task)\n        \n        # Task queue\n        self.task_queue = []\n        self.current_task = None\n        \n    def assign_task(self):\n        # Generate a random task if queue is empty\n        if not self.task_queue:\n            # Possible tasks: restock, pack_order, inspect_area\n            task_types = ['restock', 'pack_order', 'inspect_area']\n            locations = list(self.locations.keys())\n            \n            for _ in range(3):  # Add 3 tasks to queue\n                task_type = random.choice(task_types)\n                location = random.choice(locations)\n                self.task_queue.append({\n                    'type': task_type,\n                    'location': location,\n                    'priority': random.randint(1, 5)\n                })\n        \n        # Sort tasks by priority (higher number = higher priority)\n        self.task_queue.sort(key=lambda x: x['priority'], reverse=True)\n        \n        # If no current task, start the highest priority one\n        if self.current_task is None and self.task_queue:\n            self.current_task = self.task_queue.pop(0)\n            self.execute_task(self.current_task)\n    \n    def execute_task(self, task):\n        self.get_logger().info(f'Executing task: {task[\"type\"]} at {task[\"location\"]}')\n        \n        # Send navigation goal\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = 'map'\n        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()\n        \n        loc = self.locations[task['location']]\n        goal_msg.pose.pose.position.x = loc['x']\n        goal_msg.pose.pose.position.y = loc['y']\n        goal_msg.pose.pose.position.z = 0.0\n        \n        # Set orientation (simple approach)\n        from math import cos, sin\n        theta = loc['theta']\n        goal_msg.pose.pose.orientation.z = sin(theta / 2.0)\n        goal_msg.pose.pose.orientation.w = cos(theta / 2.0)\n        \n        # Wait for action server\n        self.nav_client.wait_for_server()\n        \n        # Send goal\n        self._send_goal_future = self.nav_client.send_goal_async(\n            goal_msg,\n            feedback_callback=self.feedback_callback\n        )\n        \n        self._send_goal_future.add_done_callback(self.goal_response_callback)\n    \n    def goal_response_callback(self, future):\n        goal_handle = future.result()\n        if not goal_handle.accepted:\n            self.get_logger().info('Goal rejected')\n            self.current_task = None\n            return\n\n        self.get_logger().info('Goal accepted')\n        \n        self._get_result_future = goal_handle.get_result_async()\n        self._get_result_future.add_done_callback(self.get_result_callback)\n    \n    def get_result_callback(self, future):\n        result = future.result().result\n        status = future.result().status\n        \n        if status == GoalStatus.STATUS_SUCCEEDED:\n            self.get_logger().info('Goal succeeded')\n            # Task completed, set current task to None to trigger next task\n            self.current_task = None\n        else:\n            self.get_logger().info(f'Goal failed with status: {status}')\n            # Retry the same task\n            self.execute_task(self.current_task)\n    \n    def feedback_callback(self, feedback_msg):\n        feedback = feedback_msg.feedback\n        self.get_logger().info(f'Distance remaining: {feedback.distance_remaining:.2f}m')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = WarehouseTaskPlanner()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(e.h2,{id:"launch-system",children:"Launch System"}),"\n",(0,o.jsx)(e.h3,{id:"main-launch-file",children:"Main Launch File"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# launch/warehouse_simulation.launch.py\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.conditions import IfCondition\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\n    show_gui = LaunchConfiguration('show_gui', default='true')\n    \n    # Gazebo launch\n    gazebo = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                FindPackageShare('gazebo_ros'),\n                'launch',\n                'gazebo.launch.py'\n            ])\n        ]),\n        launch_arguments={\n            'world': PathJoinSubstitution([\n                FindPackageShare('warehouse_simulation'),\n                'worlds',\n                'warehouse.world'\n            ])\n        }.items()\n    )\n    \n    # Robot spawn node\n    spawn_entity = Node(\n        package='gazebo_ros',\n        executable='spawn_entity.py',\n        arguments=[\n            '-topic', 'robot_description',\n            '-entity', 'humanoid_robot',\n            '-x', '0', '-y', '0', '-z', '1'\n        ],\n        output='screen'\n    )\n    \n    # Robot state publisher\n    robot_state_publisher = Node(\n        package='robot_state_publisher',\n        executable='robot_state_publisher',\n        name='robot_state_publisher',\n        output='screen',\n        parameters=[\n            PathJoinSubstitution([\n                FindPackageShare('warehouse_simulation'),\n                'config',\n                'robot_state_publisher.yaml'\n            ])\n        ]\n    )\n    \n    # Joint state broadcaster\n    joint_state_broadcaster_spawner = Node(\n        package='controller_manager',\n        executable='spawner',\n        arguments=['joint_state_broadcaster'],\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n    \n    # Joint trajectory controller\n    joint_trajectory_controller_spawner = Node(\n        package='controller_manager',\n        executable='spawner',\n        arguments=['joint_trajectory_controller'],\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n    \n    # Navigation\n    navigation = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                FindPackageShare('nav2_bringup'),\n                'launch',\n                'navigation_launch.py'\n            ])\n        ]),\n        launch_arguments={\n            'use_sim_time': use_sim_time\n        }.items()\n    )\n    \n    # Object detection node\n    object_detection = Node(\n        package='warehouse_simulation',\n        executable='object_detection_node',\n        name='object_detection',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n    \n    # Task planner\n    task_planner = Node(\n        package='warehouse_simulation',\n        executable='warehouse_task_planner',\n        name='task_planner',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n    \n    return LaunchDescription([\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='true',\n            description='Use simulation (Gazebo) clock if true'\n        ),\n        DeclareLaunchArgument(\n            'show_gui',\n            default_value='true',\n            description='Show Gazebo GUI if true'\n        ),\n        gazebo,\n        spawn_entity,\n        robot_state_publisher,\n        joint_state_broadcaster_spawner,\n        joint_trajectory_controller_spawner,\n        navigation,\n        object_detection,\n        task_planner\n    ])\n"})}),"\n",(0,o.jsx)(e.h2,{id:"validation-and-testing",children:"Validation and Testing"}),"\n",(0,o.jsx)(e.h3,{id:"simulation-validation",children:"Simulation Validation"}),"\n",(0,o.jsx)(e.p,{children:"To validate the complete simulation environment:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Physics Validation"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Verify robot dynamics match expected behavior"}),"\n",(0,o.jsx)(e.li,{children:"Test collision detection and response"}),"\n",(0,o.jsx)(e.li,{children:"Validate sensor data accuracy"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Integration Validation"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Confirm data flows correctly between systems"}),"\n",(0,o.jsx)(e.li,{children:"Verify Unity visualization matches Gazebo physics"}),"\n",(0,o.jsx)(e.li,{children:"Test ROS 2 communication reliability"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Functional Validation"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Execute complete tasks in the warehouse environment"}),"\n",(0,o.jsx)(e.li,{children:"Test navigation and obstacle avoidance"}),"\n",(0,o.jsx)(e.li,{children:"Validate perception system performance"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"performance-testing",children:"Performance Testing"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Measure simulation real-time factor (RTF)"}),"\n",(0,o.jsx)(e.li,{children:"Monitor CPU and memory usage"}),"\n",(0,o.jsx)(e.li,{children:"Test with varying numbers of objects and robots"}),"\n",(0,o.jsx)(e.li,{children:"Validate network communication performance"}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"This complete simulation environment example demonstrates how to integrate all the components discussed in previous chapters to create a functional, realistic simulation for humanoid robots operating in a warehouse setting. The example includes all necessary configuration files, code implementations, and launch systems to run the complete simulation environment."})]})}function u(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(m,{...n})}):m(n)}}}]);