"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[8389],{4770(n,e,a){a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>c,frontMatter:()=>r,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"module-2-digital-twin/chapter-4-sensor-simulation/synthetic-datasets","title":"Synthetic Dataset Generation Examples","description":"Introduction to Synthetic Dataset Generation","source":"@site/docs/module-2-digital-twin/chapter-4-sensor-simulation/synthetic-datasets.md","sourceDirName":"module-2-digital-twin/chapter-4-sensor-simulation","slug":"/module-2-digital-twin/chapter-4-sensor-simulation/synthetic-datasets","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-2-digital-twin/chapter-4-sensor-simulation/synthetic-datasets","draft":false,"unlisted":false,"editUrl":"https://github.com/Muhammad-Bilal-Hussain/Book-Physical-AI-Humanoid-Robotics/docs/module-2-digital-twin/chapter-4-sensor-simulation/synthetic-datasets.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: References and Citations","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-2-digital-twin/chapter-4-sensor-simulation/references"},"next":{"title":"Content Validation: Flesch-Kincaid Grade Level Assessment","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-2-digital-twin/chapter-4-sensor-simulation/validation-report"}}');var i=a(4848),o=a(8453);const r={},s="Synthetic Dataset Generation Examples",l={},d=[{value:"Introduction to Synthetic Dataset Generation",id:"introduction-to-synthetic-dataset-generation",level:2},{value:"LiDAR Dataset Generation",id:"lidar-dataset-generation",level:2},{value:"Point Cloud Dataset for Object Detection",id:"point-cloud-dataset-for-object-detection",level:3},{value:"Example 1: Urban Scene Generation",id:"example-1-urban-scene-generation",level:4},{value:"Example 2: Indoor Navigation Dataset",id:"example-2-indoor-navigation-dataset",level:4},{value:"Camera Dataset Generation",id:"camera-dataset-generation",level:2},{value:"RGB Dataset for Object Recognition",id:"rgb-dataset-for-object-recognition",level:3},{value:"Example 3: Synthetic Object Dataset",id:"example-3-synthetic-object-dataset",level:4},{value:"Depth Dataset Generation",id:"depth-dataset-generation",level:3},{value:"Example 4: Synthetic Depth Dataset",id:"example-4-synthetic-depth-dataset",level:4},{value:"IMU Dataset Generation",id:"imu-dataset-generation",level:2},{value:"Example 5: Synthetic IMU Dataset",id:"example-5-synthetic-imu-dataset",level:3},{value:"Multi-Sensor Fusion Dataset",id:"multi-sensor-fusion-dataset",level:2},{value:"Example 6: LiDAR-Camera Fusion Dataset",id:"example-6-lidar-camera-fusion-dataset",level:3},{value:"Dataset Validation and Quality Assessment",id:"dataset-validation-and-quality-assessment",level:2},{value:"Example 7: Dataset Validation Tools",id:"example-7-dataset-validation-tools",level:3}];function p(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",...(0,o.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"synthetic-dataset-generation-examples",children:"Synthetic Dataset Generation Examples"})}),"\n",(0,i.jsx)(e.h2,{id:"introduction-to-synthetic-dataset-generation",children:"Introduction to Synthetic Dataset Generation"}),"\n",(0,i.jsx)(e.p,{children:"Synthetic dataset generation is the process of creating artificial data that mimics real-world sensor data for training AI perception systems. This chapter provides practical examples of how to generate synthetic datasets for various robotic sensors, including LiDAR, cameras, and IMUs. These examples demonstrate techniques for creating diverse, annotated datasets that can be used to train robust perception algorithms."}),"\n",(0,i.jsx)(e.h2,{id:"lidar-dataset-generation",children:"LiDAR Dataset Generation"}),"\n",(0,i.jsx)(e.h3,{id:"point-cloud-dataset-for-object-detection",children:"Point Cloud Dataset for Object Detection"}),"\n",(0,i.jsx)(e.h4,{id:"example-1-urban-scene-generation",children:"Example 1: Urban Scene Generation"}),"\n",(0,i.jsx)(e.p,{children:"Creating a synthetic dataset for urban LiDAR object detection:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import numpy as np\nimport open3d as o3d\nfrom scipy.spatial.transform import Rotation as R\nimport random\n\nclass UrbanLiDARGenerator:\n    def __init__(self):\n        self.scene_bounds = {\n            'x': (-50, 50),\n            'y': (-50, 50),\n            'z': (0, 10)\n        }\n        \n    def generate_car(self, position, orientation=None):\n        \"\"\"Generate a synthetic car point cloud\"\"\"\n        if orientation is None:\n            orientation = [0, 0, 0]  # No rotation\n        \n        # Car dimensions (length, width, height)\n        length, width, height = 4.0, 1.8, 1.5\n        \n        # Create points for a car\n        x = np.random.uniform(-length/2, length/2, 500)\n        y = np.random.uniform(-width/2, width/2, 500)\n        z = np.random.uniform(0, height, 500)\n        \n        # Stack points\n        points = np.column_stack((x, y, z))\n        \n        # Apply rotation\n        rot = R.from_euler('xyz', orientation, degrees=True)\n        points = rot.apply(points)\n        \n        # Apply translation\n        points += position\n        \n        # Add some noise\n        noise = np.random.normal(0, 0.02, points.shape)\n        points += noise\n        \n        return points, 'car'\n    \n    def generate_pedestrian(self, position):\n        \"\"\"Generate a synthetic pedestrian point cloud\"\"\"\n        # Create a simple human-like shape\n        height = 1.7\n        width = 0.5\n        \n        # Head\n        head_radius = 0.15\n        head_points = []\n        for _ in range(100):\n            r = np.random.uniform(0, head_radius)\n            theta = np.random.uniform(0, 2*np.pi)\n            phi = np.random.uniform(0, np.pi)\n            x = r * np.sin(phi) * np.cos(theta)\n            y = r * np.sin(phi) * np.sin(theta)\n            z = r * np.cos(phi) + height - head_radius\n            head_points.append([x, y, z])\n        \n        # Body (cylinder)\n        body_points = []\n        for _ in range(200):\n            r = np.random.uniform(0, width/2)\n            theta = np.random.uniform(0, 2*np.pi)\n            x = r * np.cos(theta)\n            y = r * np.sin(theta)\n            z = np.random.uniform(0.2, height - head_radius)\n            body_points.append([x, y, z])\n        \n        points = np.array(head_points + body_points)\n        points += position\n        \n        # Add noise\n        noise = np.random.normal(0, 0.01, points.shape)\n        points += noise\n        \n        return points, 'pedestrian'\n    \n    def generate_scene(self, num_cars=5, num_pedestrians=10):\n        \"\"\"Generate a complete urban scene\"\"\"\n        all_points = []\n        all_labels = []\n        \n        # Generate cars\n        for i in range(num_cars):\n            x = np.random.uniform(self.scene_bounds['x'][0], \n                                 self.scene_bounds['x'][1])\n            y = np.random.uniform(self.scene_bounds['y'][0], \n                                 self.scene_bounds['y'][1])\n            z = 0  # Ground level\n            position = [x, y, z]\n            \n            car_points, label = self.generate_car(position)\n            all_points.append(car_points)\n            all_labels.extend([label] * len(car_points))\n        \n        # Generate pedestrians\n        for i in range(num_pedestrians):\n            x = np.random.uniform(self.scene_bounds['x'][0], \n                                 self.scene_bounds['x'][1])\n            y = np.random.uniform(self.scene_bounds['y'][0], \n                                 self.scene_bounds['y'][1])\n            z = 0  # Ground level\n            position = [x, y, z]\n            \n            ped_points, label = self.generate_pedestrian(position)\n            all_points.append(ped_points)\n            all_labels.extend([label] * len(ped_points))\n        \n        # Combine all points\n        combined_points = np.vstack(all_points)\n        \n        # Add ground plane\n        ground_x = np.random.uniform(self.scene_bounds['x'][0], \n                                    self.scene_bounds['x'][1], 1000)\n        ground_y = np.random.uniform(self.scene_bounds['y'][0], \n                                    self.scene_bounds['y'][1], 1000)\n        ground_z = np.random.uniform(-0.1, 0.1, 1000)  # Slightly below ground\n        ground_points = np.column_stack((ground_x, ground_y, ground_z))\n        \n        combined_points = np.vstack([combined_points, ground_points])\n        all_labels.extend(['ground'] * len(ground_points))\n        \n        return combined_points, all_labels\n\n# Example usage\ngenerator = UrbanLiDARGenerator()\npoints, labels = generator.generate_scene(num_cars=3, num_pedestrians=5)\n\n# Save the dataset\nnp.savez('urban_lidar_dataset.npz', points=points, labels=labels)\nprint(f\"Generated dataset with {len(points)} points\")\n"})}),"\n",(0,i.jsx)(e.h4,{id:"example-2-indoor-navigation-dataset",children:"Example 2: Indoor Navigation Dataset"}),"\n",(0,i.jsx)(e.p,{children:"Creating a synthetic dataset for indoor LiDAR navigation:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import numpy as np\nimport random\n\nclass IndoorLiDARGenerator:\n    def __init__(self):\n        self.room_bounds = {\n            'x': (0, 10),\n            'y': (0, 8),\n            'z': (0, 3)\n        }\n        \n    def generate_room_layout(self):\n        \"\"\"Generate walls and basic room structure\"\"\"\n        points = []\n        \n        # Generate floor\n        floor_x = np.random.uniform(self.room_bounds['x'][0], \n                                   self.room_bounds['x'][1], 2000)\n        floor_y = np.random.uniform(self.room_bounds['y'][0], \n                                   self.room_bounds['y'][1], 2000)\n        floor_z = np.random.uniform(-0.05, 0.05, 2000)  # Floor with slight variation\n        floor_points = np.column_stack((floor_x, floor_y, floor_z))\n        points.extend(floor_points)\n        \n        # Generate ceiling\n        ceil_x = np.random.uniform(self.room_bounds['x'][0], \n                                  self.room_bounds['x'][1], 2000)\n        ceil_y = np.random.uniform(self.room_bounds['y'][0], \n                                  self.room_bounds['y'][1], 2000)\n        ceil_z = np.random.uniform(self.room_bounds['z'][1]-0.05, \n                                  self.room_bounds['z'][1]+0.05, 2000)\n        ceil_points = np.column_stack((ceil_x, ceil_y, ceil_z))\n        points.extend(ceil_points)\n        \n        # Generate walls\n        # Wall 1 (x = min_x)\n        wall1_x = np.random.uniform(self.room_bounds['x'][0]-0.05, \n                                   self.room_bounds['x'][0]+0.05, 1000)\n        wall1_y = np.random.uniform(self.room_bounds['y'][0], \n                                   self.room_bounds['y'][1], 1000)\n        wall1_z = np.random.uniform(0, self.room_bounds['z'][1], 1000)\n        wall1_points = np.column_stack((wall1_x, wall1_y, wall1_z))\n        points.extend(wall1_points)\n        \n        # Wall 2 (x = max_x)\n        wall2_x = np.random.uniform(self.room_bounds['x'][1]-0.05, \n                                   self.room_bounds['x'][1]+0.05, 1000)\n        wall2_y = np.random.uniform(self.room_bounds['y'][0], \n                                   self.room_bounds['y'][1], 1000)\n        wall2_z = np.random.uniform(0, self.room_bounds['z'][1], 1000)\n        wall2_points = np.column_stack((wall2_x, wall2_y, wall2_z))\n        points.extend(wall2_points)\n        \n        # Wall 3 (y = min_y)\n        wall3_x = np.random.uniform(self.room_bounds['x'][0], \n                                   self.room_bounds['x'][1], 1000)\n        wall3_y = np.random.uniform(self.room_bounds['y'][0]-0.05, \n                                   self.room_bounds['y'][0]+0.05, 1000)\n        wall3_z = np.random.uniform(0, self.room_bounds['z'][1], 1000)\n        wall3_points = np.column_stack((wall3_x, wall3_y, wall3_z))\n        points.extend(wall3_points)\n        \n        # Wall 4 (y = max_y)\n        wall4_x = np.random.uniform(self.room_bounds['x'][0], \n                                   self.room_bounds['x'][1], 1000)\n        wall4_y = np.random.uniform(self.room_bounds['y'][1]-0.05, \n                                   self.room_bounds['y'][1]+0.05, 1000)\n        wall4_z = np.random.uniform(0, self.room_bounds['z'][1], 1000)\n        wall4_points = np.column_stack((wall4_x, wall4_y, wall4_z))\n        points.extend(wall4_points)\n        \n        return np.array(points)\n    \n    def generate_furniture(self, room_points):\n        \"\"\"Add furniture to the room\"\"\"\n        furniture_points = []\n        \n        # Generate tables\n        for _ in range(3):\n            # Table top\n            table_x = np.random.uniform(1, 8, 200)\n            table_y = np.random.uniform(1, 6, 200)\n            table_z = np.random.uniform(0.7, 0.75, 200)  # Standard table height\n            table_top = np.column_stack((table_x, table_y, table_z))\n            furniture_points.extend(table_top)\n            \n            # Table legs\n            for leg_x_offset in [-0.4, 0.4]:\n                for leg_y_offset in [-0.3, 0.3]:\n                    leg_x = table_x[:50] + leg_x_offset\n                    leg_y = table_y[:50] + leg_y_offset\n                    leg_z = np.random.uniform(0.1, 0.7, 50)\n                    leg_points = np.column_stack((leg_x, leg_y, leg_z))\n                    furniture_points.extend(leg_points)\n        \n        # Generate chairs\n        for _ in range(5):\n            # Chair seat\n            chair_x = np.random.uniform(1, 8, 100)\n            chair_y = np.random.uniform(1, 6, 100)\n            chair_z = np.random.uniform(0.45, 0.5, 100)\n            chair_seat = np.column_stack((chair_x, chair_y, chair_z))\n            furniture_points.extend(chair_seat)\n            \n            # Chair back\n            back_x = chair_x[:50]\n            back_y = chair_y[:50]\n            back_z = np.random.uniform(0.5, 0.9, 50)\n            chair_back = np.column_stack((back_x, back_y, back_z))\n            furniture_points.extend(chair_back)\n        \n        return np.array(furniture_points)\n    \n    def generate_dataset(self, num_scenes=10):\n        \"\"\"Generate multiple indoor scenes\"\"\"\n        all_scenes = []\n        \n        for scene_id in range(num_scenes):\n            room_points = self.generate_room_layout()\n            furniture_points = self.generate_furniture(room_points)\n            \n            # Combine room and furniture\n            scene_points = np.vstack([room_points, furniture_points])\n            \n            # Add some noise to simulate sensor imperfections\n            noise = np.random.normal(0, 0.01, scene_points.shape)\n            scene_points += noise\n            \n            all_scenes.append(scene_points)\n        \n        return all_scenes\n\n# Example usage\nindoor_gen = IndoorLiDARGenerator()\nindoor_scenes = indoor_gen.generate_dataset(num_scenes=5)\n\n# Save the dataset\nfor i, scene in enumerate(indoor_scenes):\n    np.save(f'indoor_scene_{i}.npy', scene)\n    \nprint(f\"Generated {len(indoor_scenes)} indoor scenes\")\n"})}),"\n",(0,i.jsx)(e.h2,{id:"camera-dataset-generation",children:"Camera Dataset Generation"}),"\n",(0,i.jsx)(e.h3,{id:"rgb-dataset-for-object-recognition",children:"RGB Dataset for Object Recognition"}),"\n",(0,i.jsx)(e.h4,{id:"example-3-synthetic-object-dataset",children:"Example 3: Synthetic Object Dataset"}),"\n",(0,i.jsx)(e.p,{children:"Creating a synthetic RGB dataset for object recognition:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import cv2\nimport numpy as np\nimport random\nfrom PIL import Image, ImageDraw, ImageFont\n\nclass SyntheticRGBGenerator:\n    def __init__(self):\n        self.object_shapes = ['circle', 'rectangle', 'triangle', 'star']\n        self.colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), \n                      (255, 0, 255), (0, 255, 255), (128, 0, 128)]\n        self.backgrounds = ['plain', 'grid', 'textured']\n    \n    def generate_background(self, width=640, height=480, bg_type='plain'):\n        \"\"\"Generate different types of backgrounds\"\"\"\n        if bg_type == 'plain':\n            bg = np.random.randint(200, 255, (height, width, 3), dtype=np.uint8)\n        elif bg_type == 'grid':\n            bg = np.random.randint(200, 255, (height, width, 3), dtype=np.uint8)\n            # Add grid pattern\n            for i in range(0, height, 30):\n                cv2.line(bg, (0, i), (width, i), (180, 180, 180), 1)\n            for i in range(0, width, 30):\n                cv2.line(bg, (i, 0), (i, height), (180, 180, 180), 1)\n        elif bg_type == 'textured':\n            bg = np.random.randint(180, 240, (height, width, 3), dtype=np.uint8)\n            # Add some texture\n            for _ in range(100):\n                x = random.randint(0, width-1)\n                y = random.randint(0, height-1)\n                color = tuple(random.randint(150, 200) for _ in range(3))\n                cv2.circle(bg, (x, y), 1, color, -1)\n        \n        return bg\n    \n    def draw_shape(self, img, shape, x, y, size, color):\n        \"\"\"Draw a shape on the image\"\"\"\n        overlay = img.copy()\n        \n        if shape == 'circle':\n            cv2.circle(overlay, (x, y), size, color, -1)\n        elif shape == 'rectangle':\n            pt1 = (x - size, y - size)\n            pt2 = (x + size, y + size)\n            cv2.rectangle(overlay, pt1, pt2, color, -1)\n        elif shape == 'triangle':\n            points = np.array([\n                [x, y - size],\n                [x - size, y + size],\n                [x + size, y + size]\n            ], np.int32)\n            cv2.fillPoly(overlay, [points], color)\n        elif shape == 'star':\n            # Draw a simple 5-pointed star\n            points = []\n            for i in range(10):\n                angle = i * np.pi / 5\n                radius = size if i % 2 == 0 else size // 2\n                px = x + int(radius * np.cos(angle - np.pi/2))\n                py = y + int(radius * np.sin(angle - np.pi/2))\n                points.append([px, py])\n            points = np.array(points, np.int32)\n            cv2.fillPoly(overlay, [points], color)\n        \n        # Apply some transparency\n        alpha = 0.8\n        img = cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0)\n        return img\n    \n    def generate_image(self, width=640, height=480):\n        \"\"\"Generate a synthetic image with objects\"\"\"\n        # Select background type\n        bg_type = random.choice(self.backgrounds)\n        img = self.generate_background(width, height, bg_type)\n        \n        # Add multiple objects\n        num_objects = random.randint(3, 8)\n        annotations = []\n        \n        for _ in range(num_objects):\n            shape = random.choice(self.object_shapes)\n            color = random.choice(self.colors)\n            size = random.randint(20, 60)\n            \n            # Position object randomly but avoid edges\n            x = random.randint(size, width - size)\n            y = random.randint(size, height - size)\n            \n            # Draw the object\n            img = self.draw_shape(img, shape, x, y, size, color)\n            \n            # Add annotation\n            annotations.append({\n                'shape': shape,\n                'color': color,\n                'bbox': [x-size, y-size, x+size, y+size],\n                'center': [x, y]\n            })\n        \n        # Add some noise to simulate real camera\n        noise = np.random.normal(0, 5, img.shape).astype(np.uint8)\n        img = cv2.add(img, noise)\n        \n        # Apply slight blur to simulate camera focus\n        img = cv2.GaussianBlur(img, (3, 3), 0)\n        \n        return img, annotations\n    \n    def generate_dataset(self, num_images=100, width=640, height=480):\n        \"\"\"Generate a complete dataset\"\"\"\n        images = []\n        annotations = []\n        \n        for i in range(num_images):\n            img, ann = self.generate_image(width, height)\n            images.append(img)\n            annotations.append(ann)\n            \n            # Save image\n            cv2.imwrite(f'synthetic_rgb_{i:03d}.png', img)\n            \n            # Save annotation\n            with open(f'synthetic_rgb_{i:03d}.txt', 'w') as f:\n                for obj in ann:\n                    f.write(f\"{obj['shape']} {obj['bbox'][0]} {obj['bbox'][1]} {obj['bbox'][2]} {obj['bbox'][3]}\\n\")\n        \n        return images, annotations\n\n# Example usage\nrgb_gen = SyntheticRGBGenerator()\nimages, annotations = rgb_gen.generate_dataset(num_images=10)\n\nprint(f\"Generated {len(images)} synthetic RGB images\")\n"})}),"\n",(0,i.jsx)(e.h3,{id:"depth-dataset-generation",children:"Depth Dataset Generation"}),"\n",(0,i.jsx)(e.h4,{id:"example-4-synthetic-depth-dataset",children:"Example 4: Synthetic Depth Dataset"}),"\n",(0,i.jsx)(e.p,{children:"Creating a synthetic depth dataset for depth estimation:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import numpy as np\nimport cv2\nfrom PIL import Image\n\nclass SyntheticDepthGenerator:\n    def __init__(self):\n        self.scene_types = [\'indoor\', \'outdoor\', \'cluttered\']\n    \n    def generate_depth_map(self, width=640, height=480, scene_type=\'indoor\'):\n        """Generate a synthetic depth map"""\n        depth_map = np.zeros((height, width), dtype=np.float32)\n        \n        if scene_type == \'indoor\':\n            # Create a simple indoor scene with objects at different depths\n            # Background wall\n            depth_map[:, :] = 5.0  # 5 meters to back wall\n            \n            # Add objects at different depths\n            # Table in the middle\n            center_x, center_y = width // 2, height // 2\n            table_size_x, table_size_y = 100, 80\n            table_depth = 2.0  # 2 meters from camera\n            depth_map[center_y-table_size_y//2:center_y+table_size_y//2,\n                     center_x-table_size_x//2:center_x+table_size_x//2] = table_depth\n            \n            # Chair in front of table\n            chair_depth = 1.5  # 1.5 meters from camera\n            depth_map[center_y+table_size_y//2:center_y+table_size_y//2+60,\n                     center_x-40:center_x+40] = chair_depth\n            \n            # Objects on table\n            for _ in range(5):\n                obj_x = np.random.randint(center_x-table_size_x//2+10, \n                                         center_x+table_size_x//2-10)\n                obj_y = np.random.randint(center_y-table_size_y//2+10, \n                                         center_y+table_size_y//2-10)\n                obj_size = np.random.randint(10, 20)\n                obj_depth = table_depth - 0.1  # Slightly in front of table\n                depth_map[obj_y-obj_size:obj_y+obj_size, \n                         obj_x-obj_size:obj_x+obj_size] = obj_depth\n        \n        elif scene_type == \'outdoor\':\n            # Create a simple outdoor scene\n            # Ground plane with varying depth\n            for y in range(height):\n                depth_map[y, :] = 1.0 + (y / height) * 10.0  # Ground slopes away\n            \n            # Add a building in the background\n            building_width = 200\n            building_start = (width - building_width) // 2\n            building_depth = 8.0\n            depth_map[height//3:, building_start:building_start+building_width] = building_depth\n            \n            # Add trees\n            for _ in range(3):\n                tree_x = np.random.randint(50, width-50)\n                tree_width = 30\n                tree_depth = np.random.uniform(3.0, 6.0)\n                depth_map[height//2:, tree_x-tree_width//2:tree_x+tree_width//2] = tree_depth\n        \n        elif scene_type == \'cluttered\':\n            # Create a cluttered scene with many objects\n            depth_map[:, :] = 10.0  # Far background\n            \n            # Add multiple objects at different depths\n            for _ in range(15):\n                obj_x = np.random.randint(20, width-20)\n                obj_y = np.random.randint(20, height-20)\n                obj_size = np.random.randint(15, 50)\n                obj_depth = np.random.uniform(0.5, 8.0)\n                \n                # Create a circular object\n                y, x = np.ogrid[:height, :width]\n                mask = (x - obj_x)**2 + (y - obj_y)**2 <= obj_size**2\n                depth_map[mask] = obj_depth\n        \n        # Add some noise to make it more realistic\n        noise = np.random.normal(0, 0.02, depth_map.shape).astype(np.float32)\n        depth_map += noise\n        \n        # Ensure depth values are positive\n        depth_map = np.maximum(depth_map, 0.1)\n        \n        return depth_map\n    \n    def generate_multiview_depth(self, width=640, height=480):\n        """Generate depth maps from multiple viewpoints"""\n        # Generate base depth map\n        base_depth = self.generate_depth_map(width, height)\n        \n        # Generate depth maps from slightly different viewpoints\n        viewpoints = []\n        for i in range(5):  # 5 different viewpoints\n            # Simulate camera movement\n            dx = np.random.uniform(-0.1, 0.1)  # Small translation\n            dy = np.random.uniform(-0.1, 0.1)\n            \n            # Create transformation matrix for small translation\n            M = np.float32([[1, 0, dx * width], [0, 1, dy * height]])\n            \n            # Apply transformation\n            transformed_depth = cv2.warpAffine(base_depth, M, (width, height), \n                                              flags=cv2.INTER_LINEAR, \n                                              borderMode=cv2.BORDER_REFLECT)\n            \n            viewpoints.append(transformed_depth)\n        \n        return viewpoints\n    \n    def generate_dataset(self, num_scenes=20):\n        """Generate a complete depth dataset"""\n        for i in range(num_scenes):\n            scene_type = random.choice(self.scene_types)\n            depth_map = self.generate_depth_map(scene_type=scene_type)\n            \n            # Convert to 16-bit format for saving (in millimeters)\n            depth_mm = (depth_map * 1000).astype(np.uint16)\n            \n            # Save depth map\n            depth_img = Image.fromarray(depth_mm)\n            depth_img.save(f\'depth_map_{i:03d}.png\')\n            \n            # Also save as numpy array\n            np.save(f\'depth_map_{i:03d}.npy\', depth_map)\n        \n        print(f"Generated {num_scenes} synthetic depth maps")\n\n# Example usage\ndepth_gen = SyntheticDepthGenerator()\ndepth_gen.generate_dataset(num_scenes=5)\n'})}),"\n",(0,i.jsx)(e.h2,{id:"imu-dataset-generation",children:"IMU Dataset Generation"}),"\n",(0,i.jsx)(e.h3,{id:"example-5-synthetic-imu-dataset",children:"Example 5: Synthetic IMU Dataset"}),"\n",(0,i.jsx)(e.p,{children:"Creating a synthetic IMU dataset for motion analysis:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.transform import Rotation as R\n\nclass SyntheticIMUMotionGenerator:\n    def __init__(self, sample_rate=100):  # 100 Hz sample rate\n        self.sample_rate = sample_rate\n        self.dt = 1.0 / sample_rate\n    \n    def generate_linear_motion(self, duration=10.0, acceleration_profile='constant'):\n        \"\"\"Generate linear motion with known acceleration\"\"\"\n        num_samples = int(duration * self.sample_rate)\n        time = np.linspace(0, duration, num_samples)\n        \n        if acceleration_profile == 'constant':\n            # Constant acceleration\n            true_acc = np.array([0.5, 0.0, -9.8])  # Including gravity\n            acceleration = np.tile(true_acc, (num_samples, 1))\n            \n        elif acceleration_profile == 'sine':\n            # Sine wave acceleration\n            freq = 0.5  # 0.5 Hz\n            amplitude = 2.0\n            x_acc = amplitude * np.sin(2 * np.pi * freq * time)\n            y_acc = amplitude * np.cos(2 * np.pi * freq * time)\n            z_acc = -9.8 + 0.5 * np.sin(2 * np.pi * freq * time * 0.7)  # Gravity + small variation\n            acceleration = np.column_stack((x_acc, y_acc, z_acc))\n        \n        # Generate angular velocity (should be zero for pure linear motion)\n        angular_velocity = np.zeros((num_samples, 3))\n        \n        return acceleration, angular_velocity, time\n    \n    def generate_rotational_motion(self, duration=10.0):\n        \"\"\"Generate rotational motion\"\"\"\n        num_samples = int(duration * self.sample_rate)\n        time = np.linspace(0, duration, num_samples)\n        \n        # Define rotation profile\n        omega_x = 0.5 * np.sin(0.3 * time)  # Roll\n        omega_y = 0.3 * np.cos(0.4 * time)  # Pitch  \n        omega_z = 0.2 * np.sin(0.5 * time)  # Yaw\n        \n        angular_velocity = np.column_stack((omega_x, omega_y, omega_z))\n        \n        # Calculate orientation by integrating angular velocity\n        orientation = np.zeros((num_samples, 4))  # Quaternion\n        current_quat = np.array([1.0, 0.0, 0.0, 0.0])  # Identity quaternion\n        \n        for i in range(1, num_samples):\n            # Convert angular velocity to quaternion derivative\n            omega = angular_velocity[i-1]\n            omega_norm = np.linalg.norm(omega)\n            \n            if omega_norm > 1e-6:  # Avoid division by zero\n                axis = omega / omega_norm\n                angle = omega_norm * self.dt\n                \n                # Create rotation quaternion\n                dq = np.array([\n                    np.cos(angle/2),\n                    axis[0] * np.sin(angle/2),\n                    axis[1] * np.sin(angle/2), \n                    axis[2] * np.sin(angle/2)\n                ])\n                \n                # Update orientation\n                current_quat = self.quat_multiply(dq, current_quat)\n                current_quat = current_quat / np.linalg.norm(current_quat)\n            \n            orientation[i] = current_quat\n        \n        # Calculate linear acceleration from rotation (centripetal and tangential)\n        acceleration = np.zeros((num_samples, 3))\n        \n        # For simplicity, assume a point mass at distance r from rotation center\n        r = 0.5  # 0.5m from center\n        for i in range(num_samples):\n            omega = angular_velocity[i]\n            # Simplified model: centripetal acceleration = omega^2 * r\n            omega_mag = np.linalg.norm(omega)\n            if omega_mag > 1e-6:\n                # Centripetal acceleration direction is perpendicular to rotation axis and position\n                # For simplicity, we'll just use a simplified model\n                acc_mag = omega_mag**2 * r\n                # Direction is perpendicular to rotation axis\n                acc_dir = np.array([-omega[1], omega[0], 0])  # Perpendicular in xy-plane\n                if np.linalg.norm(acc_dir) > 1e-6:\n                    acc_dir = acc_dir / np.linalg.norm(acc_dir)\n                    acceleration[i] = acc_dir * acc_mag\n            # Add gravity\n            acceleration[i, 2] -= 9.81\n        \n        return acceleration, angular_velocity, time, orientation\n    \n    def quat_multiply(self, q1, q2):\n        \"\"\"Multiply two quaternions\"\"\"\n        w1, x1, y1, z1 = q1\n        w2, x2, y2, z2 = q2\n        \n        w = w1*w2 - x1*x2 - y1*y2 - z1*z2\n        x = w1*x2 + x1*w2 + y1*z2 - z1*y2\n        y = w1*y2 - x1*z2 + y1*w2 + z1*x2\n        z = w1*z2 + x1*y2 - y1*x2 + z1*w2\n        \n        return np.array([w, x, y, z])\n    \n    def add_imu_noise(self, true_acc, true_gyro, noise_params=None):\n        \"\"\"Add realistic IMU noise to the signals\"\"\"\n        if noise_params is None:\n            # Default noise parameters (typical for a good IMU)\n            noise_params = {\n                'acc_white': 0.017,      # m/s^2 / sqrt(Hz) (ADIS16448 typical)\n                'acc_bias_instability': 25e-6 * 9.81,  # m/s^2\n                'gyro_white': 0.0035,    # rad/s / sqrt(Hz)\n                'gyro_bias_instability': 3.5e-5,       # rad/s\n            }\n        \n        num_samples = len(true_acc)\n        \n        # Generate noise for accelerometers\n        acc_white_noise = np.random.normal(0, noise_params['acc_white'] * np.sqrt(self.sample_rate), \n                                          (num_samples, 3))\n        # Add bias instability (random walk)\n        acc_bias = np.random.normal(0, noise_params['acc_bias_instability'], (num_samples, 3))\n        acc_bias = np.cumsum(acc_bias, axis=0)  # Random walk\n        \n        # Generate noise for gyroscopes\n        gyro_white_noise = np.random.normal(0, noise_params['gyro_white'] * np.sqrt(self.sample_rate), \n                                           (num_samples, 3))\n        # Add bias instability (random walk)\n        gyro_bias = np.random.normal(0, noise_params['gyro_bias_instability'], (num_samples, 3))\n        gyro_bias = np.cumsum(gyro_bias, axis=0)  # Random walk\n        \n        # Add noise to true signals\n        noisy_acc = true_acc + acc_white_noise + acc_bias\n        noisy_gyro = true_gyro + gyro_white_noise + gyro_bias\n        \n        return noisy_acc, noisy_gyro\n    \n    def generate_dataset(self, num_trajectories=5):\n        \"\"\"Generate multiple IMU trajectories\"\"\"\n        for traj_id in range(num_trajectories):\n            print(f\"Generating trajectory {traj_id+1}/{num_trajectories}\")\n            \n            # Generate different types of motion\n            motion_type = np.random.choice(['linear', 'rotational'])\n            \n            if motion_type == 'linear':\n                acc, gyro, time = self.generate_linear_motion(duration=10.0, \n                                                            acceleration_profile='sine')\n            else:  # rotational\n                acc, gyro, time, orientation = self.generate_rotational_motion(duration=10.0)\n            \n            # Add realistic IMU noise\n            noisy_acc, noisy_gyro = self.add_imu_noise(acc, gyro)\n            \n            # Save the data\n            data_dict = {\n                'time': time,\n                'true_acceleration': acc,\n                'noisy_acceleration': noisy_acc,\n                'true_angular_velocity': gyro,\n                'noisy_angular_velocity': noisy_gyro,\n                'motion_type': motion_type\n            }\n            \n            if motion_type == 'rotational':\n                data_dict['orientation'] = orientation\n            \n            np.savez(f'imu_trajectory_{traj_id:02d}.npz', **data_dict)\n        \n        print(f\"Generated {num_trajectories} synthetic IMU trajectories\")\n\n# Example usage\nimu_gen = SyntheticIMUMotionGenerator(sample_rate=100)\nimu_gen.generate_dataset(num_trajectories=3)\n"})}),"\n",(0,i.jsx)(e.h2,{id:"multi-sensor-fusion-dataset",children:"Multi-Sensor Fusion Dataset"}),"\n",(0,i.jsx)(e.h3,{id:"example-6-lidar-camera-fusion-dataset",children:"Example 6: LiDAR-Camera Fusion Dataset"}),"\n",(0,i.jsx)(e.p,{children:"Creating a synthetic dataset that combines LiDAR and camera data:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import numpy as np\nimport cv2\nfrom scipy.spatial.transform import Rotation as R\n\nclass MultiSensorFusionGenerator:\n    def __init__(self):\n        # Camera intrinsic parameters\n        self.fx = 554.0\n        self.fy = 554.0\n        self.cx = 320.0\n        self.cy = 240.0\n        \n        # Camera-LiDAR extrinsic calibration (rotation and translation)\n        # This represents the transformation from LiDAR frame to camera frame\n        self.R_lidar_to_cam = R.from_euler('xyz', [0, 0, np.pi/2]).as_matrix()  # 90-degree rotation\n        self.t_lidar_to_cam = np.array([0.1, 0.05, 0.05])  # 10cm x, 5cm y, 5cm z offset\n    \n    def project_lidar_to_camera(self, points_3d):\n        \"\"\"Project 3D LiDAR points to 2D camera image coordinates\"\"\"\n        # Transform points from LiDAR frame to camera frame\n        points_cam = (self.R_lidar_to_cam @ points_3d.T).T + self.t_lidar_to_cam\n        \n        # Remove points behind the camera\n        valid_idx = points_cam[:, 2] > 0\n        points_cam = points_cam[valid_idx]\n        \n        # Project to 2D image coordinates\n        x = points_cam[:, 0] * self.fx / points_cam[:, 2] + self.cx\n        y = points_cam[:, 1] * self.fy / points_cam[:, 2] + self.cy\n        \n        # Create image coordinates and depth\n        u = x.astype(int)\n        v = y.astype(int)\n        depth = points_cam[:, 2]\n        \n        # Filter points within image bounds\n        valid_coords = (u >= 0) & (u < 640) & (v >= 0) & (v < 480)\n        \n        return {\n            'u': u[valid_coords],\n            'v': v[valid_coords], \n            'depth': depth[valid_coords],\n            'points_3d': points_cam[valid_coords]\n        }\n    \n    def generate_fusion_scene(self, num_objects=5):\n        \"\"\"Generate a scene with both LiDAR and camera data\"\"\"\n        # Generate LiDAR point cloud\n        lidar_gen = UrbanLiDARGenerator()\n        points_3d, labels = lidar_gen.generate_scene(num_cars=2, num_pedestrians=3)\n        \n        # Project LiDAR points to camera\n        projection = self.project_lidar_to_camera(points_3d)\n        \n        # Generate corresponding camera image\n        # Create a simple background\n        img = np.random.randint(200, 255, (480, 640, 3), dtype=np.uint8)\n        \n        # Draw projected LiDAR points on the image\n        for u, v, depth in zip(projection['u'], projection['v'], projection['depth']):\n            # Color based on depth\n            color_intensity = int(255 * (1 - min(depth / 50.0, 1.0)))\n            color = (color_intensity, 0, 255 - color_intensity)  # Blue-red gradient\n            \n            # Draw point\n            cv2.circle(img, (int(u), int(v)), 2, color, -1)\n        \n        # Add some noise to the image\n        noise = np.random.normal(0, 10, img.shape).astype(np.uint8)\n        img = cv2.add(img, noise)\n        \n        return {\n            'lidar_points': points_3d,\n            'lidar_labels': labels,\n            'camera_image': img,\n            'projected_points': projection,\n            'calibration': {\n                'intrinsic': np.array([[self.fx, 0, self.cx],\n                                      [0, self.fy, self.cy], \n                                      [0, 0, 1]]),\n                'extrinsic_rot': self.R_lidar_to_cam,\n                'extrinsic_trans': self.t_lidar_to_cam\n            }\n        }\n    \n    def generate_fusion_dataset(self, num_scenes=10):\n        \"\"\"Generate a multi-sensor fusion dataset\"\"\"\n        for i in range(num_scenes):\n            scene_data = self.generate_fusion_scene()\n            \n            # Save LiDAR data\n            np.savez(f'fusion_lidar_{i:03d}.npz',\n                    points=scene_data['lidar_points'],\n                    labels=scene_data['lidar_labels'])\n            \n            # Save camera data\n            cv2.imwrite(f'fusion_camera_{i:03d}.png', scene_data['camera_image'])\n            \n            # Save projected points\n            np.savez(f'fusion_projection_{i:03d}.npz',\n                    u=scene_data['projected_points']['u'],\n                    v=scene_data['projected_points']['v'],\n                    depth=scene_data['projected_points']['depth'])\n            \n            # Save calibration\n            np.savez(f'fusion_calibration_{i:03d}.npz',\n                    intrinsic=scene_data['calibration']['intrinsic'],\n                    extrinsic_rot=scene_data['calibration']['extrinsic_rot'],\n                    extrinsic_trans=scene_data['calibration']['extrinsic_trans'])\n        \n        print(f\"Generated {num_scenes} multi-sensor fusion scenes\")\n\n# Example usage\nfusion_gen = MultiSensorFusionGenerator()\nfusion_gen.generate_fusion_dataset(num_scenes=5)\n"})}),"\n",(0,i.jsx)(e.h2,{id:"dataset-validation-and-quality-assessment",children:"Dataset Validation and Quality Assessment"}),"\n",(0,i.jsx)(e.h3,{id:"example-7-dataset-validation-tools",children:"Example 7: Dataset Validation Tools"}),"\n",(0,i.jsx)(e.p,{children:"Creating tools to validate synthetic datasets:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import numpy as np\nimport matplotlib.pyplot as plt\n\nclass DatasetValidator:\n    @staticmethod\n    def validate_lidar_dataset(file_path):\n        """Validate a LiDAR dataset"""\n        data = np.load(file_path)\n        points = data[\'points\']\n        labels = data[\'labels\'] if \'labels\' in data.files else None\n        \n        print(f"Lidar Dataset Validation:")\n        print(f"  - Number of points: {len(points)}")\n        print(f"  - Point dimensions: {points.shape[1]}")\n        print(f"  - X range: [{points[:, 0].min():.2f}, {points[:, 0].max():.2f}]")\n        print(f"  - Y range: [{points[:, 1].min():.2f}, {points[:, 1].max():.2f}]")\n        print(f"  - Z range: [{points[:, 2].min():.2f}, {points[:, 2].max():.2f}]")\n        \n        if labels is not None:\n            unique_labels, counts = np.unique(labels, return_counts=True)\n            print(f"  - Labels: {dict(zip(unique_labels, counts))}")\n        \n        # Check for NaN or infinite values\n        nan_count = np.sum(np.isnan(points))\n        inf_count = np.sum(np.isinf(points))\n        print(f"  - NaN values: {nan_count}")\n        print(f"  - Infinite values: {inf_count}")\n        \n        return nan_count == 0 and inf_count == 0\n    \n    @staticmethod\n    def validate_camera_dataset(image_path, annotation_path=None):\n        """Validate a camera dataset"""\n        import cv2\n        \n        img = cv2.imread(image_path)\n        if img is None:\n            print(f"Error: Could not load image {image_path}")\n            return False\n        \n        print(f"Camera Dataset Validation:")\n        print(f"  - Image shape: {img.shape}")\n        print(f"  - Data type: {img.dtype}")\n        print(f"  - Value range: [{img.min()}, {img.max()}]")\n        \n        # Check for valid pixel values\n        if img.dtype == np.uint8:\n            valid_range = (img >= 0).all() and (img <= 255).all()\n        else:\n            valid_range = (img >= 0).all() and (img <= 1.0).all()\n        \n        print(f"  - Valid pixel range: {valid_range}")\n        \n        return valid_range\n    \n    @staticmethod\n    def validate_imu_dataset(file_path):\n        """Validate an IMU dataset"""\n        data = np.load(file_path)\n        \n        print(f"IMU Dataset Validation:")\n        \n        for key in [\'time\', \'noisy_acceleration\', \'noisy_angular_velocity\']:\n            if key in data:\n                arr = data[key]\n                print(f"  - {key}: shape {arr.shape}, range [{arr.min():.4f}, {arr.max():.4f}]")\n        \n        # Check for valid IMU data characteristics\n        acc_magnitude = np.linalg.norm(data[\'noisy_acceleration\'], axis=1)\n        gyro_magnitude = np.linalg.norm(data[\'noisy_angular_velocity\'], axis=1)\n        \n        print(f"  - Acc magnitude range: [{acc_magnitude.min():.4f}, {acc_magnitude.max():.4f}] m/s\xb2")\n        print(f"  - Gyro magnitude range: [{gyro_magnitude.min():.4f}, {gyro_magnitude.max():.4f}] rad/s")\n        \n        # Check for reasonable ranges\n        reasonable_acc = (acc_magnitude < 100).all()  # Shouldn\'t exceed 100 m/s\xb2 in normal conditions\n        reasonable_gyro = (gyro_magnitude < 100).all()  # Shouldn\'t exceed 100 rad/s in normal conditions\n        \n        print(f"  - Reasonable acceleration range: {reasonable_acc}")\n        print(f"  - Reasonable angular velocity range: {reasonable_gyro}")\n        \n        return reasonable_acc and reasonable_gyro\n\n# Example usage of validation\nvalidator = DatasetValidator()\n\n# Validate a few generated files\nlidar_valid = validator.validate_lidar_dataset(\'urban_lidar_dataset.npz\')\nprint(f"Lidar dataset valid: {lidar_valid}\\n")\n\nimu_files = [f\'imu_trajectory_{i:02d}.npz\' for i in range(3)]\nfor imu_file in imu_files:\n    try:\n        imu_valid = validator.validate_imu_dataset(imu_file)\n        print(f"IMU dataset {imu_file} valid: {imu_valid}\\n")\n    except FileNotFoundError:\n        print(f"IMU file {imu_file} not found\\n")\n'})}),"\n",(0,i.jsx)(e.p,{children:"These examples demonstrate various approaches to generating synthetic datasets for different types of robotic sensors. The examples include:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"LiDAR dataset generation for object detection in urban and indoor environments"}),"\n",(0,i.jsx)(e.li,{children:"Camera dataset generation for object recognition with various backgrounds"}),"\n",(0,i.jsx)(e.li,{children:"Depth dataset generation for depth estimation tasks"}),"\n",(0,i.jsx)(e.li,{children:"IMU dataset generation with realistic noise models"}),"\n",(0,i.jsx)(e.li,{children:"Multi-sensor fusion dataset combining LiDAR and camera data"}),"\n",(0,i.jsx)(e.li,{children:"Dataset validation tools to ensure quality"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"Each example includes realistic noise models, proper calibration, and validation techniques to ensure the synthetic data is suitable for training AI perception systems."})]})}function c(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(p,{...n})}):p(n)}},8453(n,e,a){a.d(e,{R:()=>r,x:()=>s});var t=a(6540);const i={},o=t.createContext(i);function r(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:r(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);