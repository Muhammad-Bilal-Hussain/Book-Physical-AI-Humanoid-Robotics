"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[6492],{5214(e,n,i){i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-4-vla-robotics/chapter-3-llm-cognitive-planning","title":"Chapter 3: LLM-Based Cognitive Planning for ROS 2","description":"Large Language Models in Robotics Applications","source":"@site/docs/module-4-vla-robotics/chapter-3-llm-cognitive-planning.md","sourceDirName":"module-4-vla-robotics","slug":"/module-4-vla-robotics/chapter-3-llm-cognitive-planning","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/chapter-3-llm-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/Muhammad-Bilal-Hussain/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/chapter-3-llm-cognitive-planning.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Voice-to-Action Pipelines with OpenAI Whisper","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/chapter-2-voice-to-action"},"next":{"title":"Chapter 4: Capstone: The Autonomous Humanoid","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/chapter-4-autonomous-humanoid"}}');var s=i(4848),l=i(8453);const t={},o="Chapter 3: LLM-Based Cognitive Planning for ROS 2",r={},c=[{value:"Large Language Models in Robotics Applications",id:"large-language-models-in-robotics-applications",level:2},{value:"Capabilities of LLMs in Robotics",id:"capabilities-of-llms-in-robotics",level:3},{value:"Challenges and Limitations",id:"challenges-and-limitations",level:3},{value:"Mapping Natural Language to ROS 2 Actions",id:"mapping-natural-language-to-ros-2-actions",level:2},{value:"The Translation Challenge",id:"the-translation-challenge",level:3},{value:"Example Translation Process",id:"example-translation-process",level:3},{value:"Template-Based Approaches",id:"template-based-approaches",level:3},{value:"Semantic Parsing",id:"semantic-parsing",level:3},{value:"Planning Hierarchies and Task Decomposition",id:"planning-hierarchies-and-task-decomposition",level:2},{value:"Hierarchical Task Networks",id:"hierarchical-task-networks",level:3},{value:"Example Hierarchy",id:"example-hierarchy",level:3},{value:"Dynamic Replanning",id:"dynamic-replanning",level:3},{value:"Safety and Validation of LLM-Generated Plans",id:"safety-and-validation-of-llm-generated-plans",level:2},{value:"Safety Challenges",id:"safety-challenges",level:3},{value:"Validation Approaches",id:"validation-approaches",level:3},{value:"Safety-First Architecture",id:"safety-first-architecture",level:3},{value:"Formal Verification",id:"formal-verification",level:3},{value:"Case Studies of LLM-ROS Integration",id:"case-studies-of-llm-ros-integration",level:2},{value:"RT-2: Robotics Transformer 2",id:"rt-2-robotics-transformer-2",level:3},{value:"PaLM-E",id:"palm-e",level:3},{value:"VoxPoser",id:"voxposer",level:3},{value:"Cognitive Planning Module Architecture",id:"cognitive-planning-module-architecture",level:2},{value:"System Components",id:"system-components",level:3},{value:"Architecture Pattern",id:"architecture-pattern",level:3},{value:"Knowledge Integration",id:"knowledge-integration",level:3},{value:"Symbolic Grounding of Language in Physical Space",id:"symbolic-grounding-of-language-in-physical-space",level:2},{value:"The Grounding Problem",id:"the-grounding-problem",level:3},{value:"Approaches to Grounding",id:"approaches-to-grounding",level:3},{value:"Vision-Language Models",id:"vision-language-models",level:3},{value:"Spatial Reasoning",id:"spatial-reasoning",level:3},{value:"References",id:"references",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-3-llm-based-cognitive-planning-for-ros-2",children:"Chapter 3: LLM-Based Cognitive Planning for ROS 2"})}),"\n",(0,s.jsx)(n.h2,{id:"large-language-models-in-robotics-applications",children:"Large Language Models in Robotics Applications"}),"\n",(0,s.jsx)(n.p,{children:"Large Language Models (LLMs) have revolutionized the field of robotics by providing unprecedented capabilities for natural language understanding and high-level reasoning. These models, trained on vast amounts of text data, can interpret complex human instructions and generate detailed plans for robotic systems."}),"\n",(0,s.jsx)(n.h3,{id:"capabilities-of-llms-in-robotics",children:"Capabilities of LLMs in Robotics"}),"\n",(0,s.jsx)(n.p,{children:"LLMs bring several key capabilities to robotics:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Natural Language Understanding"}),": Ability to comprehend complex, nuanced human instructions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"World Knowledge"}),": Access to vast amounts of general knowledge that can inform robotic decision-making"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reasoning"}),": Capacity for logical reasoning and problem-solving in novel situations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Generalization"}),": Ability to apply learned knowledge to new, unseen situations"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"challenges-and-limitations",children:"Challenges and Limitations"}),"\n",(0,s.jsx)(n.p,{children:"Despite their impressive capabilities, LLMs face several challenges in robotics applications:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hallucinations"}),": Tendency to generate plausible-sounding but incorrect information"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Lack of Real-time Awareness"}),": Models trained on static data may not reflect current environmental conditions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Physical Grounding"}),": Difficulty connecting abstract language concepts to physical reality"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety and Reliability"}),": Potential for generating unsafe or infeasible plans"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"mapping-natural-language-to-ros-2-actions",children:"Mapping Natural Language to ROS 2 Actions"}),"\n",(0,s.jsx)(n.h3,{id:"the-translation-challenge",children:"The Translation Challenge"}),"\n",(0,s.jsx)(n.p,{children:"One of the key challenges in LLM-robotics integration is translating high-level natural language commands into specific ROS 2 actions. This translation process involves several steps:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Intent Recognition"}),": Determining the high-level goal from the natural language command"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Entity Extraction"}),": Identifying relevant objects, locations, and parameters"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Selection"}),": Choosing appropriate ROS 2 actions to achieve the goal"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Parameter Mapping"}),": Converting natural language parameters to ROS 2 action parameters"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"example-translation-process",children:"Example Translation Process"}),"\n",(0,s.jsx)(n.p,{children:'Consider the command "Pick up the red cup from the table and put it in the sink":'}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Intent Recognition"}),": Two-part task - pick up object, then place object"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Entity Extraction"}),': Object: "red cup", Source: "table", Destination: "sink"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Selection"}),": Use MoveIt! for navigation, Gripper action for pickup, Place action for placement"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Parameter Mapping"}),': Convert "red cup" to object recognition parameters, "table" and "sink" to navigation goals']}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"template-based-approaches",children:"Template-Based Approaches"}),"\n",(0,s.jsx)(n.p,{children:"One approach to mapping natural language to ROS 2 actions involves using predefined templates:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'Template: "Move [object] from [location1] to [location2]"\nMaps to: Navigate to location1 -> Detect object -> Grasp object -> Navigate to location2 -> Release object\n'})}),"\n",(0,s.jsx)(n.h3,{id:"semantic-parsing",children:"Semantic Parsing"}),"\n",(0,s.jsx)(n.p,{children:"More sophisticated approaches use semantic parsing to convert natural language into formal representations that can be directly translated to ROS 2 actions. This involves:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Syntactic Analysis"}),": Parsing the sentence structure"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Semantic Role Labeling"}),": Identifying the roles of different entities in the action"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Logical Form Generation"}),": Creating a formal representation of the command"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Mapping"}),": Converting the logical form to ROS 2 actions"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"planning-hierarchies-and-task-decomposition",children:"Planning Hierarchies and Task Decomposition"}),"\n",(0,s.jsx)(n.h3,{id:"hierarchical-task-networks",children:"Hierarchical Task Networks"}),"\n",(0,s.jsx)(n.p,{children:"LLMs excel at decomposing complex tasks into hierarchical structures. This decomposition typically follows a top-down approach:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"High-Level Goals"}),": Abstract goals specified in natural language"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Mid-Level Tasks"}),": More specific tasks that contribute to achieving the high-level goal"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Low-Level Actions"}),": Specific ROS 2 actions that can be directly executed"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"example-hierarchy",children:"Example Hierarchy"}),"\n",(0,s.jsx)(n.p,{children:'For the command "Clean the kitchen," an LLM might generate the following hierarchy:'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Level 1: Clean the kitchen\n\u251c\u2500\u2500 Level 2: Clear the counter\n\u2502   \u251c\u2500\u2500 Level 3: Identify objects on counter\n\u2502   \u251c\u2500\u2500 Level 3: Pick up dishes\n\u2502   \u2514\u2500\u2500 Level 3: Place dishes in dishwasher\n\u251c\u2500\u2500 Level 2: Wipe the counter\n\u2502   \u251c\u2500\u2500 Level 3: Navigate to counter\n\u2502   \u251c\u2500\u2500 Level 3: Grasp cleaning cloth\n\u2502   \u2514\u2500\u2500 Level 3: Execute wiping motions\n\u2514\u2500\u2500 Level 2: Sweep the floor\n    \u251c\u2500\u2500 Level 3: Navigate to storage\n    \u251c\u2500\u2500 Level 3: Grasp broom\n    \u2514\u2500\u2500 Level 3: Execute sweeping motions\n"})}),"\n",(0,s.jsx)(n.h3,{id:"dynamic-replanning",children:"Dynamic Replanning"}),"\n",(0,s.jsx)(n.p,{children:"Hierarchical planning allows for dynamic replanning when unexpected situations arise. If a low-level action fails, the system can attempt alternative approaches at the same level or request higher-level guidance from the LLM."}),"\n",(0,s.jsx)(n.h2,{id:"safety-and-validation-of-llm-generated-plans",children:"Safety and Validation of LLM-Generated Plans"}),"\n",(0,s.jsx)(n.h3,{id:"safety-challenges",children:"Safety Challenges"}),"\n",(0,s.jsx)(n.p,{children:"LLMs can generate plans that are unsafe or infeasible for robotic systems. Key safety challenges include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Physical Impossibility"}),": Plans that violate laws of physics or robot kinematics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Environmental Hazards"}),": Plans that lead the robot into dangerous situations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Social Norms"}),": Plans that violate social or cultural norms"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resource Constraints"}),": Plans that exceed robot capabilities"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"validation-approaches",children:"Validation Approaches"}),"\n",(0,s.jsx)(n.p,{children:"Several approaches can be used to validate LLM-generated plans:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simulation Testing"}),": Test plans in simulation before execution"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Rule-Based Checking"}),": Apply predefined safety rules to filter plans"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Expert Validation"}),": Use domain experts to review plans"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Incremental Execution"}),": Execute plans in small increments with safety checks"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"safety-first-architecture",children:"Safety-First Architecture"}),"\n",(0,s.jsx)(n.p,{children:"A safety-first architecture places validation layers between the LLM and the robot:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"[LLM-Generated Plan]\n         \u2193\n[Safety Validator]\n         \u2193\n[Feasibility Checker]\n         \u2193\n[Simulation Validator]\n         \u2193\n[Robot Execution]\n"})}),"\n",(0,s.jsx)(n.h3,{id:"formal-verification",children:"Formal Verification"}),"\n",(0,s.jsx)(n.p,{children:"For critical applications, formal verification methods can be used to mathematically prove that LLM-generated plans satisfy safety properties. This involves:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Formal Specification"}),": Defining safety properties in formal logic"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Plan Translation"}),": Converting the plan to a formal representation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verification"}),": Using automated tools to check safety properties"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"case-studies-of-llm-ros-integration",children:"Case Studies of LLM-ROS Integration"}),"\n",(0,s.jsx)(n.h3,{id:"rt-2-robotics-transformer-2",children:"RT-2: Robotics Transformer 2"}),"\n",(0,s.jsx)(n.p,{children:"RT-2 represents a significant advancement in LLM-robotics integration. This system:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Combines vision-language models with robotic control"}),"\n",(0,s.jsx)(n.li,{children:"Learns robotic skills from internet data"}),"\n",(0,s.jsx)(n.li,{children:"Can generalize to novel situations and objects"}),"\n",(0,s.jsx)(n.li,{children:"Uses transformer architecture for end-to-end learning"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"palm-e",children:"PaLM-E"}),"\n",(0,s.jsx)(n.p,{children:"PaLM-E integrates a large language model with embodied robotic systems:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Provides contextual understanding for robotic tasks"}),"\n",(0,s.jsx)(n.li,{children:"Handles ambiguous or underspecified commands"}),"\n",(0,s.jsx)(n.li,{children:"Demonstrates strong performance on complex manipulation tasks"}),"\n",(0,s.jsx)(n.li,{children:"Incorporates real-time perception feedback"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"voxposer",children:"VoxPoser"}),"\n",(0,s.jsx)(n.p,{children:"VoxPoser enables robots to manipulate objects based on natural language descriptions:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Uses 3D spatial reasoning to understand object relationships"}),"\n",(0,s.jsx)(n.li,{children:"Generates precise manipulation plans from language descriptions"}),"\n",(0,s.jsx)(n.li,{children:"Integrates with ROS 2 for execution"}),"\n",(0,s.jsx)(n.li,{children:"Demonstrates strong performance on challenging manipulation tasks"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"cognitive-planning-module-architecture",children:"Cognitive Planning Module Architecture"}),"\n",(0,s.jsx)(n.h3,{id:"system-components",children:"System Components"}),"\n",(0,s.jsx)(n.p,{children:"A typical cognitive planning module for LLM-robotics integration includes:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Language Interface"}),": Handles communication with the LLM"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Knowledge Base"}),": Stores information about the environment and robot capabilities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Planning Engine"}),": Decomposes high-level goals into executable actions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Validation Layer"}),": Ensures safety and feasibility of generated plans"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Execution Monitor"}),": Tracks plan execution and handles exceptions"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"architecture-pattern",children:"Architecture Pattern"}),"\n",(0,s.jsx)(n.p,{children:"The cognitive planning module typically follows this pattern:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"[Language Command]\n         \u2193\n[LLM Interpretation]\n         \u2193\n[Knowledge Integration]\n         \u2193\n[Hierarchical Planning]\n         \u2193\n[Safety Validation]\n         \u2193\n[ROS 2 Action Generation]\n         \u2193\n[Execution Monitoring]\n         \u2193\n[Feedback Integration]\n"})}),"\n",(0,s.jsx)(n.h3,{id:"knowledge-integration",children:"Knowledge Integration"}),"\n",(0,s.jsx)(n.p,{children:"The planning module integrates various types of knowledge:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"World Knowledge"}),": General knowledge from the LLM"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Spatial Knowledge"}),": Information about object locations and relationships"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robot Knowledge"}),": Information about robot capabilities and limitations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Task Knowledge"}),": Information about specific tasks and procedures"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"symbolic-grounding-of-language-in-physical-space",children:"Symbolic Grounding of Language in Physical Space"}),"\n",(0,s.jsx)(n.h3,{id:"the-grounding-problem",children:"The Grounding Problem"}),"\n",(0,s.jsx)(n.p,{children:"Symbolic grounding refers to the challenge of connecting abstract language concepts to physical reality. This is crucial for robotics applications where language commands must be executed in the real world."}),"\n",(0,s.jsx)(n.h3,{id:"approaches-to-grounding",children:"Approaches to Grounding"}),"\n",(0,s.jsx)(n.p,{children:"Several approaches address the grounding problem:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Perceptual Grounding"}),": Connect language to perceptual features (visual, auditory, tactile)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Functional Grounding"}),": Connect language to functional properties and affordances"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Interactive Grounding"}),": Learn grounding through interaction with the environment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Distributional Grounding"}),": Use statistical patterns in language and perception"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"vision-language-models",children:"Vision-Language Models"}),"\n",(0,s.jsx)(n.p,{children:"Vision-language models help bridge the gap between language and perception:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CLIP"}),": Connects visual and textual representations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"BLIP"}),": Provides bidirectional vision-language understanding"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Florence"}),": Offers comprehensive vision-language representations"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"spatial-reasoning",children:"Spatial Reasoning"}),"\n",(0,s.jsx)(n.p,{children:"Spatial reasoning is crucial for grounding language in physical space:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Coordinate Systems"}),": Map language references to spatial coordinates"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Topological Relations"}),": Understand spatial relationships (above, below, next to)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Size and Scale"}),": Ground language references to physical dimensions"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["OpenAI. (2023). ",(0,s.jsx)(n.em,{children:"GPT-4 Technical Report"}),". arXiv preprint arXiv:2303.08774."]}),"\n",(0,s.jsxs)(n.li,{children:["Google Research. (2022). ",(0,s.jsx)(n.em,{children:"RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"}),". arXiv preprint arXiv:2212.06817."]}),"\n",(0,s.jsxs)(n.li,{children:["Google Research. (2022). ",(0,s.jsx)(n.em,{children:"PaLM-E: An Embodied Multimodal Language Model"}),". arXiv preprint arXiv:2203.03560."]}),"\n",(0,s.jsx)(n.li,{children:"[To be filled with peer-reviewed papers on LLMs in robotics]"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>t,x:()=>o});var a=i(6540);const s={},l=a.createContext(s);function t(e){const n=a.useContext(l);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),a.createElement(l.Provider,{value:n},e.children)}}}]);