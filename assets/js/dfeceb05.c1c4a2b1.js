"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[3897],{2965(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"module-4-vla-robotics/architecture-diagram","title":"Architecture Diagram: Vision-Language-Action (VLA) System","description":"Layered Architecture Overview","source":"@site/docs/module-4-vla-robotics/architecture-diagram.md","sourceDirName":"module-4-vla-robotics","slug":"/module-4-vla-robotics/architecture-diagram","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/architecture-diagram","draft":false,"unlisted":false,"editUrl":"https://github.com/Muhammad-Bilal-Hussain/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/architecture-diagram.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Transition to Module 4: Vision-Language-Action","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-3-isaac-ai-brain/transition-to-module-4"},"next":{"title":"Chapter 1: Vision-Language-Action Paradigm in Robotics","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/chapter-1-vla-paradigm"}}');var t=i(4848),o=i(8453);const s={},a="Architecture Diagram: Vision-Language-Action (VLA) System",c={},l=[{value:"Layered Architecture Overview",id:"layered-architecture-overview",level:2},{value:"Detailed Layer Breakdown",id:"detailed-layer-breakdown",level:2},{value:"1. Input Layer",id:"1-input-layer",level:3},{value:"2. Speech Layer",id:"2-speech-layer",level:3},{value:"3. Cognitive Layer",id:"3-cognitive-layer",level:3},{value:"4. Perception Layer",id:"4-perception-layer",level:3},{value:"5. Action Layer",id:"5-action-layer",level:3},{value:"6. Feedback Loop",id:"6-feedback-loop",level:3},{value:"Integration Patterns",id:"integration-patterns",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"architecture-diagram-vision-language-action-vla-system",children:"Architecture Diagram: Vision-Language-Action (VLA) System"})}),"\n",(0,t.jsx)(n.h2,{id:"layered-architecture-overview",children:"Layered Architecture Overview"}),"\n",(0,t.jsx)(n.p,{children:"The VLA system follows a layered architecture pattern:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"[Human Voice Command]\n         \u2193 (Speech-to-Text)\n[Language Processing Pipeline]\n         \u2193 (Intent Understanding)\n[Cognitive Planning Module]\n         \u2193 (Action Planning)\n[ROS 2 Action Mapping]\n         \u2193 (Command Execution)\n[Perception-Action Loop]\n         \u2193 (Physical Action)\n[Robot Action Execution]\n         \u2193 (Feedback)\n[Human Perception]\n"})}),"\n",(0,t.jsx)(n.h2,{id:"detailed-layer-breakdown",children:"Detailed Layer Breakdown"}),"\n",(0,t.jsx)(n.h3,{id:"1-input-layer",children:"1. Input Layer"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Component"}),": Human voice commands"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Function"}),": Provides natural language input to the system"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Purpose"}),": Enables intuitive human-robot interaction through speech"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-speech-layer",children:"2. Speech Layer"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Component"}),": OpenAI Whisper (speech-to-text)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Function"}),": Converts spoken language to text for processing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Purpose"}),": Enables the system to understand verbal commands"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-cognitive-layer",children:"3. Cognitive Layer"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Component"}),": Large Language Model (task reasoning & planning)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Function"}),": Interprets commands and generates high-level action plans"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Purpose"}),": Provides high-level reasoning and task decomposition"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"4-perception-layer",children:"4. Perception Layer"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Component"}),": Vision inputs from simulated sensors"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Function"}),": Provides environmental awareness and context"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Purpose"}),": Enables the robot to understand its surroundings"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"5-action-layer",children:"5. Action Layer"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Component"}),": ROS 2 action execution"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Function"}),": Translates plans into physical robot movements"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Purpose"}),": Executes the planned actions in the physical world"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"6-feedback-loop",children:"6. Feedback Loop"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Component"}),": Environment and perception updates"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Function"}),": Provides information about action outcomes"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Purpose"}),": Enables adaptive behavior and error correction"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"integration-patterns",children:"Integration Patterns"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use ROS 2 interfaces for communication between VLA components"}),"\n",(0,t.jsx)(n.li,{children:"Implement safety checks between LLM planning and action execution"}),"\n",(0,t.jsx)(n.li,{children:"Apply behavior trees for complex task orchestration"}),"\n",(0,t.jsx)(n.li,{children:"Utilize semantic mapping for grounding language in physical space"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>s,x:()=>a});var r=i(6540);const t={},o=r.createContext(t);function s(e){const n=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);