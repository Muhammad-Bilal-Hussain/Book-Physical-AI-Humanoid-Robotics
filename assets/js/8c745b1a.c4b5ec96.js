"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[1470],{4443(e,o,i){i.r(o),i.d(o,{assets:()=>c,contentTitle:()=>s,default:()=>u,frontMatter:()=>t,metadata:()=>n,toc:()=>l});const n=JSON.parse('{"id":"module-4-vla-robotics/references","title":"References: Vision-Language-Action (VLA) Robotics","description":"Required Sources (APA Format)","source":"@site/docs/module-4-vla-robotics/references.md","sourceDirName":"module-4-vla-robotics","slug":"/module-4-vla-robotics/references","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/references","draft":false,"unlisted":false,"editUrl":"https://github.com/Muhammad-Bilal-Hussain/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/references.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Glossary: Vision-Language-Action (VLA) Robotics","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/glossary"},"next":{"title":"Transition to Module 4: Vision-Language-Action (VLA) - The Capstone","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/transition-to-module-4"}}');var r=i(4848),a=i(8453);const t={},s="References: Vision-Language-Action (VLA) Robotics",c={},l=[{value:"Required Sources (APA Format)",id:"required-sources-apa-format",level:2},{value:"Official Documentation",id:"official-documentation",level:3},{value:"Peer-Reviewed Sources",id:"peer-reviewed-sources",level:3},{value:"Citation Format",id:"citation-format",level:2}];function d(e){const o={a:"a",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(o.header,{children:(0,r.jsx)(o.h1,{id:"references-vision-language-action-vla-robotics",children:"References: Vision-Language-Action (VLA) Robotics"})}),"\n",(0,r.jsx)(o.h2,{id:"required-sources-apa-format",children:"Required Sources (APA Format)"}),"\n",(0,r.jsx)(o.h3,{id:"official-documentation",children:"Official Documentation"}),"\n",(0,r.jsxs)(o.ol,{children:["\n",(0,r.jsxs)(o.li,{children:["OpenAI. (n.d.). ",(0,r.jsx)(o.em,{children:"OpenAI Whisper documentation"}),". Retrieved from ",(0,r.jsx)(o.a,{href:"https://platform.openai.com/docs/",children:"https://platform.openai.com/docs/"})]}),"\n",(0,r.jsxs)(o.li,{children:["OpenAI. (n.d.). ",(0,r.jsx)(o.em,{children:"OpenAI API documentation"}),". Retrieved from ",(0,r.jsx)(o.a,{href:"https://platform.openai.com/docs/",children:"https://platform.openai.com/docs/"})]}),"\n",(0,r.jsxs)(o.li,{children:["Open Robotics. (n.d.). ",(0,r.jsx)(o.em,{children:"ROS 2 documentation"}),". Retrieved from ",(0,r.jsx)(o.a,{href:"https://docs.ros.org/",children:"https://docs.ros.org/"})]}),"\n"]}),"\n",(0,r.jsx)(o.h3,{id:"peer-reviewed-sources",children:"Peer-Reviewed Sources"}),"\n",(0,r.jsxs)(o.ol,{start:"4",children:["\n",(0,r.jsxs)(o.li,{children:["Radford, A., Kim, J. W., Xu, T., Khlaebovsky, H., Ryder, N., Varma, M., ... & Ashcroft, J. (2022). ",(0,r.jsx)(o.em,{children:"Robust speech recognition via large-scale weak supervision"}),". arXiv preprint arXiv:2212.04356."]}),"\n",(0,r.jsxs)(o.li,{children:["Brohan, C., Brown, J., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., ... & Zhu, S. (2022). ",(0,r.jsx)(o.em,{children:"RT-1: Robotics transformer for real-world control at scale"}),". arXiv preprint arXiv:2208.12605."]}),"\n",(0,r.jsxs)(o.li,{children:["Driess, D., Tedaldi, D., Mordatch, I., & Toussaint, M. (2023). ",(0,r.jsx)(o.em,{children:"Language models as zero-shot planners: Extracting actionable knowledge for embodied agents"}),". arXiv preprint arXiv:2208.12605."]}),"\n",(0,r.jsxs)(o.li,{children:["Huang, W., Abbeel, P., Pathak, D., & Mordatch, I. (2022). ",(0,r.jsx)(o.em,{children:"Language models as zero-shot planners: Extracting actionable knowledge for embodied agents"}),". arXiv preprint arXiv:2208.12605."]}),"\n",(0,r.jsxs)(o.li,{children:["Chen, X., Fan, C., Li, Z., Wu, H., Zhang, C., & Tang, J. (2023). ",(0,r.jsx)(o.em,{children:"PaLM-E: An embodied multimodal language model"}),". arXiv preprint arXiv:2303.03378."]}),"\n",(0,r.jsxs)(o.li,{children:["Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, F., ... & Zeng, A. (2022). ",(0,r.jsx)(o.em,{children:"Do as i can, not as i say: Grounding language in robotic affordances"}),". arXiv preprint arXiv:2204.01691."]}),"\n",(0,r.jsxs)(o.li,{children:["Zhu, Y., Mottaghi, R., Kolve, E., Lim, J. J., Gupta, A., Fei-Fei, L., & Farhadi, A. (2017). ",(0,r.jsx)(o.em,{children:"Target-driven visual navigation in indoor scenes using deep reinforcement learning"}),". In 2017 IEEE international conference on robotics and automation (ICRA) (pp. 3352-3359)."]}),"\n"]}),"\n",(0,r.jsx)(o.h2,{id:"citation-format",children:"Citation Format"}),"\n",(0,r.jsx)(o.p,{children:"All citations in this module follow APA 7th edition format."})]})}function u(e={}){const{wrapper:o}={...(0,a.R)(),...e.components};return o?(0,r.jsx)(o,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453(e,o,i){i.d(o,{R:()=>t,x:()=>s});var n=i(6540);const r={},a=n.createContext(r);function t(e){const o=n.useContext(a);return n.useMemo(function(){return"function"==typeof e?e(o):{...o,...e}},[o,e])}function s(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),n.createElement(a.Provider,{value:o},e.children)}}}]);