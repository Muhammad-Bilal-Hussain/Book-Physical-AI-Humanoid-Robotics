"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[5924],{8453(n,e,s){s.d(e,{R:()=>r,x:()=>c});var o=s(6540);const i={},t=o.createContext(i);function r(n){const e=o.useContext(t);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function c(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:r(n.components),o.createElement(t.Provider,{value:e},n.children)}},8565(n,e,s){s.r(e),s.d(e,{assets:()=>a,contentTitle:()=>c,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"module-4-vla-robotics/glossary","title":"Glossary: Vision-Language-Action (VLA) Robotics","description":"Key Terms and Definitions","source":"@site/docs/module-4-vla-robotics/glossary.md","sourceDirName":"module-4-vla-robotics","slug":"/module-4-vla-robotics/glossary","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/glossary","draft":false,"unlisted":false,"editUrl":"https://github.com/Muhammad-Bilal-Hussain/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/glossary.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: Capstone: The Autonomous Humanoid","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/chapter-4-autonomous-humanoid"},"next":{"title":"References: Vision-Language-Action (VLA) Robotics","permalink":"/Book-Physical-AI-Humanoid-Robotics/docs/module-4-vla-robotics/references"}}');var i=s(4848),t=s(8453);const r={},c="Glossary: Vision-Language-Action (VLA) Robotics",a={},l=[{value:"Key Terms and Definitions",id:"key-terms-and-definitions",level:2}];function d(n){const e={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"glossary-vision-language-action-vla-robotics",children:"Glossary: Vision-Language-Action (VLA) Robotics"})}),"\n",(0,i.jsx)(e.h2,{id:"key-terms-and-definitions",children:"Key Terms and Definitions"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Vision-Language-Action (VLA)"}),": The paradigm integrating visual perception, language understanding, and physical action execution in robotics."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Large Language Model (LLM)"}),": AI models capable of understanding and generating human language, used for high-level reasoning in robotics."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Embodied AI"}),": AI systems that interact with the physical world through robotic bodies, combining perception, cognition, and action."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Cognitive Robotics"}),": A field that applies cognitive science principles to robotics, focusing on higher-level reasoning and decision-making."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Multimodal Learning"}),": Machine learning that processes and integrates information from multiple sensory modalities (e.g., vision, language, audio)."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Speech-to-Text (STT)"}),": The process of converting spoken language into written text, often using models like OpenAI's Whisper."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Automatic Speech Recognition (ASR)"}),": Technology that converts speech into text, forming the basis for voice-controlled systems."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Natural Language Processing (NLP)"}),": A field of AI focused on enabling computers to understand, interpret, and generate human language."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Perception-Action Loop"}),": The continuous cycle in robotics involving sensing the environment, processing information, and executing actions."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Symbolic Grounding"}),": The process of connecting abstract symbols (e.g., words) to concrete sensory experiences or physical actions."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Hierarchical Planning"}),": A planning approach that decomposes complex tasks into subtasks at different levels of abstraction."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Adaptive Autonomy"}),": A system design where the level of automation adjusts based on context, task complexity, or operator state."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Human-Robot Interaction (HRI)"}),": The study of interactions between humans and robots, focusing on design and implementation of robotic systems for human use."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"ROS 2 Action Mapping"}),": The process of translating high-level commands into specific ROS 2 action executions for robotic systems."]}),"\n"]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}}}]);